{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edc3e3b8-bda0-403c-a9d5-2e3afaf00b0b",
   "metadata": {},
   "source": [
    "# 1. Vector Space Models\n",
    "Throughout this chapter, we’ll represent text\n",
    "units (characters, phonemes, words, phrases, sentences, paragraphs,\n",
    "and documents) with vectors of numbers. This is known as the vector\n",
    "space model (VSM). \\\n",
    "It’s a\n",
    "mathematical model that represents text units as vectors.In this setting, the most common way to\n",
    "calculate similarity between two text blobs is using cosine similarity:\n",
    "\n",
    "![](images/similarity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8740ca45-20c2-4b1a-a022-26514ebf8b08",
   "metadata": {},
   "source": [
    "# 2. Basic Vectorization \n",
    "Let’s start with a basic idea of text representation: map each word in\n",
    "the vocabulary (V) of the text corpus to a unique ID (integer value),\n",
    "then represent each sentence or document in the corpus as a V dimensional vector.\n",
    "\n",
    "Table 3-1. Our toy corpus\n",
    "\n",
    "|    |                |\n",
    "|----|----------------|\n",
    "| D1 | Dog bites man  |\n",
    "| D2 | Man bites dog. |\n",
    "| D3 | Dog eats meat. |\n",
    "| D4 | Man eats food. |\n",
    "|    |                |\n",
    "|    |                |\n",
    "\n",
    "Lowercasing text and ignoring punctuation, the vocabulary of this\n",
    "corpus is comprised of six words: [dog, bites, man, eats, meat, food].\n",
    "Every\n",
    "document in this corpus can now be represented with a vector of size\n",
    "six. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f76e49-ce34-4e4b-81c6-e08adb0b39a5",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "Let’s understand this via our toy corpus. We first map each of the six\n",
    "words to unique IDs: dog = 1, bites = 2, man = 3, meat = 4 , food = 5,\n",
    "eats = 6. Let’s consider the document D1: “dog bites man”. As per the\n",
    "scheme, each word is a six-dimensional vector. Dog is represented as\n",
    "[1 0 0 0 0 0], as the word “dog” is mapped to ID 1. Bites is\n",
    "represented as [0 1 0 0 0 0], and so on and so forth. Thus, D1 is\n",
    "represented as [ [1 0 0 0 0 0] [0 1 0 0 0 0] [0 0 1 0 0 0]]. D4 is\n",
    "represented as [ [ 0 0 1 0 0] [0 0 0 0 1 0] [0 0 0 0 0 1]]. Other\n",
    "documents in the corpus can be represented similarly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c4722b-5370-428a-8019-0bdfc8f71d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c078b45a-4c7b-496b-8d04-379ae74192d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dog': 1, 'bites': 2, 'man': 3, 'eats': 4, 'meat': 5, 'food': 6}\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary\n",
    "vocab = {}\n",
    "count = 0\n",
    "for doc in processed_docs:\n",
    "    for word in doc.split():\n",
    "        if word not in vocab:\n",
    "            count = count + 1\n",
    "            vocab[word] = count\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e0fe93-9e18-4f27-b4fd-6f4cd3202954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get one hot representation for any string based on this vocabulary. \n",
    "#If the word exists in the vocabulary, its representation is returned. \n",
    "#If not, a list of zeroes is returned for that word. \n",
    "def get_onehot_vector(somestring):\n",
    "    onehot_encoded = []\n",
    "    for word in somestring.split():\n",
    "        temp = [0]*len(vocab)\n",
    "        if word in vocab:\n",
    "            temp[vocab[word]-1] = 1\n",
    "        onehot_encoded.append(temp)\n",
    "    return onehot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a50b774-7d09-4159-8729-697754dd578c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man bites dog\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(processed_docs[1])\n",
    "get_onehot_vector(processed_docs[1]) #one hot representation for a text from our cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fab520ae-bf6c-4bfc-98f9-739065ba5eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_onehot_vector(\"man and dog are good\") \n",
    "#one hot representation for a random text, using the above vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10c7e11-07d8-44dd-a5c3-2d40d0ae0642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_onehot_vector(\"man and man are good\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f430ef7-0ce9-4b7b-8281-9c1b530a6d16",
   "metadata": {},
   "source": [
    "### One-hot encoding using scikit -learn\n",
    "We will demostrate:\n",
    "- One Hot Encoding: In one-hot encoding, each word w in corpus vocabulary is given a unique integer id wid that is between 1 and |V|, where V is the set of corpus vocab. Each word is then represented by a V-dimensional binary vector of 0s and 1s.\n",
    "\n",
    "- Label Encoding: In Label Encoding, each word w in our corpus is converted into a numeric value between 0 and n-1 (where n refers to number of unique words in our corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c2a14dc-c681-4b21-ab16-0136f8b36496",
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = 'dog bites man'\n",
    "S2 = 'man bites dog'\n",
    "S3 = 'dog eats meat'\n",
    "S4 = 'man eats food'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8bec5d4-6d39-4cbd-b5bc-f14a03e37209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data:  [['dog', 'bites', 'man'], ['man', 'bites', 'dog'], ['dog', 'eats', 'meat'], ['man', 'eats', 'food']]\n",
      "Label Encoder:  [1 0 4 4 0 1 1 2 5 4 2 3]\n",
      "One hot matrix: \n",
      " [[1. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "data = [S1.split(), S2.split(), S3.split(), S4.split()]\n",
    "values = data[0] + data[1]+ data[2]+ data[3]\n",
    "print(\"The data: \", data)\n",
    "\n",
    "# Label Encoding\n",
    "label_encorder = LabelEncoder()\n",
    "integer_encoder = label_encorder.fit_transform(values)\n",
    "print(\"Label Encoder: \", integer_encoder)\n",
    "\n",
    "# One hot encoder\n",
    "onehot_encoder = OneHotEncoder()\n",
    "onehot_encoder = onehot_encoder.fit_transform(data).toarray()\n",
    "print(\"One hot matrix: \\n\", onehot_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012b636-9450-4581-a3cf-cf9201cb8f1e",
   "metadata": {},
   "source": [
    "**Few shortcomings of one hot encodering**\n",
    "- The size of a one-hot vector is directly proportional to size of\n",
    "the vocabulary, and most real-world corpora have large\n",
    "vocabularies.\n",
    "- This representation does not give a fixed-length\n",
    "representation for text.\n",
    "- It treats words as atomic units and has no notion of\n",
    "(dis)similarity between words.\n",
    "- Out of vocabulary (OOV) problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c7344-a438-494e-9bff-42e47744a49b",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "The key idea behind it is as follows:\n",
    "represent the text under consideration as a bag (collection) of words\n",
    "while ignoring the order and context. \n",
    "\n",
    "Thus, for our toy corpus (Table 3-1), where the word IDs are dog = 1,\n",
    "bites = 2, man = 3, meat = 4 , food = 5, eats = 6, D1 becomes [1 1 1 0\n",
    "0 0]. This is because the first three words in the vocabulary appeared\n",
    "exactly once in D1, and the last three did not appear at all. D4\n",
    "becomes [0 0 1 0 1 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e7c66c-9d85-4378-848b-70ce20c2c019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"] #Same as the earlier notebook\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec9558bf-be33-4397-93f8-1c1c03c42d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our corpus:  ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n",
      "Our vocabulary:  {'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n",
      "Bow representation for 'dog bites man':  [[1 1 0 0 1 0]]\n",
      "Bow representation for 'man bites dog':  [[1 1 0 0 1 0]]\n",
      "Bow representation for 'dog and dog are friends': [[0 2 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# look at document list\n",
    "print(\"Our corpus: \", processed_docs)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "\n",
    "# Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#see the BOW rep for first 2 documents\n",
    "print(\"Bow representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"Bow representation for 'man bites dog': \", bow_rep[1].toarray())\n",
    "\n",
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9507f9d7-49ec-4dab-821d-e70ee55b41eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bow representation for 'dog and dog are friends': [[0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#BoW with binary vectors\n",
    "count_vect = CountVectorizer(binary=True)\n",
    "count_vect.fit(processed_docs)\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc747114-95a7-4591-8e26-9925e4a7ce0b",
   "metadata": {},
   "source": [
    "Let’s look at some of the advantages of this encoding:\n",
    "- Like one-hot encoding, BoW is fairly simple to understand\n",
    "and implement.\n",
    "- With this representation, documents having the same words\n",
    "will have their vector representations closer to each other in\n",
    "Euclidean space as compared to documents with completely\n",
    "different words.\n",
    "- We have a fixed-length encoding for any sentence of arbitrary\n",
    "length.\n",
    "\n",
    "However, it has its share of disadvantages, too:\n",
    "- The size of the vector increases with the size of the\n",
    "vocabulary.\n",
    "- It does not capture the similarity between different words that\n",
    "mean the same thing.\n",
    "- This representation does not have any way to handle out of\n",
    "vocabulary words (i.e., new words that were not seen in the\n",
    "corpus that was used to build the vectorizer).\n",
    "- As the name indicates, it is a “bag” of words—word order\n",
    "information is lost in this representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4d7fe9-6f70-4903-91ef-283f3632443e",
   "metadata": {},
   "source": [
    "## Bag of N-Grams\n",
    "The\n",
    "bag-of-n-grams (BoN) approach tries to remedy this. It does so by\n",
    "breaking text into chunks of n contiguous words (or tokens). This can\n",
    "help us capture some context, which earlier approaches could not do.\n",
    "Each chunk is called an n-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e956356a-26d1-400b-a534-0f5978022d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#our corpus\n",
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e77436b-23be-4b43-8a4f-01545bce13b7",
   "metadata": {},
   "source": [
    "CountVectorizer, which we used for BoW, can be used for getting a Bag of N-grams representation as well, using its ngram_range argument. The code snippet below shows how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d082be14-3b2d-457b-bf3b-b52ee60c0ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary:  {'dog': 3, 'bites': 0, 'man': 12, 'dog bites': 4, 'bites man': 2, 'dog bites man': 5, 'man bites': 13, 'bites dog': 1, 'man bites dog': 14, 'eats': 8, 'meat': 17, 'dog eats': 6, 'eats meat': 10, 'dog eats meat': 7, 'food': 11, 'man eats': 15, 'eats food': 9, 'man eats food': 16}\n",
      "BoW representation for 'dog bites man':  [[1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
      "BoW representation for 'man bites dog:  [[1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0]]\n",
      "Bow representation for 'dog and dog are friends': [[0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Ngram vectorization example with count vectorizer and uni, bi, trigrams\n",
    "count_vect = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "\n",
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "#see the BOW rep for first 2 documents\n",
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n",
    "\n",
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180af452-cf9e-4ef9-9281-d41b9842025e",
   "metadata": {},
   "source": [
    "Here are the main pros and cons of BoN:\n",
    "- It captures some context and word-order information in the\n",
    "form of n-grams.\n",
    "- Thus, resulting vector space is able to capture some semantic\n",
    "similarity. Documents having the same n-grams will have\n",
    "their vectors closer to each other in Euclidean space as\n",
    "compared to documents with completely different n-grams.\n",
    "- As n increases, dimensionality (and therefore sparsity) only\n",
    "increases rapidly.\n",
    "- It still provides no way to address the OOV problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ee1d8-4dbe-446f-b3b5-e45df1a596b0",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "In all the other approaches we saw so far, all the words in the text are treated equally important. There is no notion of some words in the document being more important than others. TF-IDF addresses this issue. It aims to quantify the importance of a given word relative to other words in the document and in the corpus. It was commonly used representation scheme for information retrieval systems, for extracting relevant documents from a corpus for given text query.\n",
    "\n",
    "*TF (term frequency)* measures how often a term or word occurs in a\n",
    "given document.\n",
    "\n",
    "$TF(t, d) = \\frac{(Number of occurrences of term t in document d)}{(Total number of terms in the document d)}$\n",
    "\n",
    "*IDF (inverse document frequency)* measures the importance of the\n",
    "term across a corpus. \n",
    "\n",
    "$IDF(t) = log_{e}\\frac{(Total number of documents in the corpus)\n",
    "}{(Number of documents with term t in them )}$\n",
    "\n",
    "$TF-IDF score = TF * IDF$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d84e1b9d-0ed4-4458-8baa-81dcd6b4fd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88349eb8-bb7e-47d7-9e66-5a61523428dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF for all words in the vocabulary [1.51082562 1.22314355 1.51082562 1.91629073 1.22314355 1.91629073]\n",
      "----------\n",
      "All words in the vocabulary ['bites', 'dog', 'eats', 'food', 'man', 'meat']\n",
      "----------\n",
      "TFIDF representation for all documents in our corpus\n",
      " [[0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
      " [0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
      " [0.         0.44809973 0.55349232 0.         0.         0.70203482]\n",
      " [0.         0.         0.55349232 0.70203482 0.44809973 0.        ]]\n",
      "----------\n",
      "Tfidf representation for 'dog and man are friends':\n",
      " [[0.         0.70710678 0.         0.         0.70710678 0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\minhh\\documents\\dev\\nlp\\.venv\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "bow_rep_tfidf = tfidf.fit_transform(processed_docs)\n",
    "\n",
    "#IDF for all words in the vocabulary\n",
    "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
    "print(\"-\"*10)\n",
    "#All words in the vocabulary.\n",
    "print(\"All words in the vocabulary\",tfidf.get_feature_names())\n",
    "print(\"-\"*10)\n",
    "\n",
    "#TFIDF representation for all documents in our corpus \n",
    "print(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray()) \n",
    "print(\"-\"*10)\n",
    "\n",
    "temp = tfidf.transform([\"dog and man are friends\"])\n",
    "print(\"Tfidf representation for 'dog and man are friends':\\n\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f3a419-a564-4a91-8d55-163df282b908",
   "metadata": {},
   "source": [
    "# 3. Distributed Representations\n",
    "- **Distributional similarity**: This is the idea that the meaning of a word can be understood from\n",
    "the context in which the word appears.\n",
    "- **Distributional hypothesis**: In linguistics, this hypothesizes that words that occur in similar\n",
    "contexts have similar meanings.\n",
    "- **Distributional representation**: This refers to representation schemes that are obtained based on\n",
    "distribution of words from the context in which the words appear.\n",
    "These schemes are based on distributional hypotheses.\n",
    "- **Distributed representation**: This is a related concept. It, too, is based on the distributional\n",
    "hypothesis.\n",
    "- **Embedding**: For the set of words in a corpus, embedding is a mapping between\n",
    "vector space coming from distributional representation to vector\n",
    "space coming from distributed representation.\n",
    "- **Vector semantics**: This refers to the set of NLP methods that aim to learn the word\n",
    "representations based on distributional properties of words in a\n",
    "large corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4391725-5dcf-4aa6-a562-378f004767fd",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "Word2vec ensures\n",
    "that the learned word representations are low dimensional (vectors of\n",
    "dimensions 50–500, instead of several thousands) and dense (that is, most values\n",
    "in these vectors are non-zero). Word2vec uses distributional similarity and distributional\n",
    "hypothesis. Word2vec takes a large corpus of text as input and\n",
    "“learns” to represent the words in a common vector space based on the\n",
    "contexts in which they appear in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b268fa-46c7-4992-ae49-d803e36b1000",
   "metadata": {},
   "source": [
    "### PRE-TRAINED WORD EMBEDDINGS\n",
    "Some of the most popular pre-trained embeddings are\n",
    "**Word2vec** by Google, **GloVe** by Stanford, and **fasttext**\n",
    "embeddings by Facebook, to name a few. Further, they’re\n",
    "available for various dimensions like $d = 25, 50, 100, 200, 300, 600$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad1ca551-7a6a-434c-9b45-c685167fa637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings #This module ignores the various types of warnings generated\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import psutil #This module helps in retrieving information on running processes and system resource utilization\n",
    "process = psutil.Process(os.getpid())\n",
    "from psutil import virtual_memory\n",
    "mem = virtual_memory()\n",
    "\n",
    "import time #This module is used to calculate the time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532bf1f7-4f5a-4541-9632-427e35d7c575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used in GB before Loading the Model: 0.13\n",
      "----------\n",
      "39.68 seconds taken to load\n",
      "----------\n",
      "Finished loading Word2Vec\n",
      "----------\n",
      "Memory used in GB after Loading the Model: 4.82\n",
      "----------\n",
      "Percentage increase in memory usage: 3666.09% \n",
      "----------\n",
      "Numver of words in vocablulary:  3000000\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "pretrainedpath = \"GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "#Load W2V model. This will take some time, but it is a one time effort! \n",
    "pre = process.memory_info().rss\n",
    "print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Check memory usage before loading the model\n",
    "print('-'*10)\n",
    "\n",
    "start_time = time.time() #Start the timer\n",
    "ttl = mem.total #Toal memory available\n",
    "\n",
    "w2v_model = KeyedVectors.load_word2vec_format(pretrainedpath, binary=True) #load the model\n",
    "print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Calculate the total time elapsed since starting the timer\n",
    "print('-'*10)\n",
    "\n",
    "print('Finished loading Word2Vec')\n",
    "print('-'*10)\n",
    "\n",
    "post = process.memory_info().rss\n",
    "print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Calculate the memory used after loading the model\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Percentage increase in memory usage: {:.2f}% \".format(float((post/pre)*100))) #Percentage increase in memory after loading the model\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Numver of words in vocablulary: \",len(w2v_model.vocab)) #Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "320479d8-70f3-4d5e-bea1-50611c939e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gorgeous', 0.8353004455566406),\n",
       " ('lovely', 0.810693621635437),\n",
       " ('stunningly_beautiful', 0.7329413890838623),\n",
       " ('breathtakingly_beautiful', 0.7231341004371643),\n",
       " ('wonderful', 0.6854087114334106),\n",
       " ('fabulous', 0.6700063943862915),\n",
       " ('loveliest', 0.6612576246261597),\n",
       " ('prettiest', 0.6595001816749573),\n",
       " ('beatiful', 0.6593326330184937),\n",
       " ('magnificent', 0.6591402292251587)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let us examine the model by knowing what the most similar words are, for a given word!\n",
    "w2v_model.most_similar('beautiful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d5b9b3-9e6a-48bc-91b9-7ffbedece0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What is the vector representation for a word? \n",
    "w2v_model['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6086162-3dc2-4934-9e41-8bd1eaeb009b",
   "metadata": {},
   "source": [
    "**Two things to note while using pre-trained models:**\n",
    "1. Tokens/Words are always lowercased. If a word is not in the vocabulary, the model throws an exception.\n",
    "2. So, it is always a good idea to encapsulate those statements in try/except blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c51ccd-d1de-4aa6-b79f-9aff14fb943f",
   "metadata": {},
   "source": [
    "### TRAINING OUR OWN EMBEDDINGS\n",
    "### CBOW (Continuous bag of words)\n",
    "CBOW tries to learn a language model that tries to predict the “center”\n",
    "word from the words in its context.\n",
    "\n",
    "![](images/cbow.png)\n",
    "<center>CBOW: given the context words, predict the center word</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7d47fbc-e1f4-4e93-801e-c84bdd957d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\minhh\\documents\\dev\\nlp\\.venv\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4a51953-5a06-460e-999b-6382823a6a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training data\n",
    "#Genism word2vec requires that a format of ‘list of lists’ be provided for training where every document contained in a list.\n",
    "#Every list contains lists of tokens of that document.\n",
    "corpus = [['dog','bites','man'], [\"man\", \"bites\" ,\"dog\"],[\"dog\",\"eats\",\"meat\"],[\"man\", \"eats\",\"food\"]]\n",
    "\n",
    "#Training the model\n",
    "model_cbow = Word2Vec(corpus, min_count=1,sg=0) #using CBOW Architecture for trainnig\n",
    "model_skipgram = Word2Vec(corpus, min_count=1,sg=1)#using skipGram Architecture for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d062737-ae58-41a3-b5e0-e16126d06641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=6, size=100, alpha=0.025)\n",
      "['dog', 'bites', 'man', 'eats', 'meat', 'food']\n",
      "[-2.2637864e-04  1.1613251e-03 -1.0871805e-03  2.7705024e-03\n",
      " -3.1460954e-03 -1.5414666e-03  1.0732063e-04  2.1121099e-03\n",
      "  1.3804922e-03 -8.9474044e-05 -1.9507032e-04 -3.7300987e-03\n",
      "  1.2606074e-03 -4.3903719e-04  9.7034819e-05 -2.3361982e-03\n",
      "  3.0791026e-03 -7.3196675e-04 -1.5462874e-03  4.1705086e-03\n",
      " -1.2412993e-03  1.2338976e-03 -4.3189838e-03 -1.8837935e-03\n",
      " -4.1378485e-03 -4.2353724e-03 -4.7259713e-03  2.3909356e-03\n",
      " -4.4260966e-03 -6.0296076e-04  5.2839715e-04 -4.4406243e-03\n",
      " -4.7190804e-03  4.7737407e-03  4.9759797e-03 -3.8089291e-03\n",
      "  2.2341565e-03 -2.6227571e-03  1.8058733e-04 -4.9039810e-03\n",
      "  2.0937852e-03  1.7333912e-03 -1.9353403e-03  3.7302757e-03\n",
      " -3.3402010e-03  2.8572769e-03  3.3832456e-03 -4.9053812e-03\n",
      " -2.1320502e-03  1.7676475e-03 -4.2440277e-03 -2.0944851e-03\n",
      " -4.5314971e-03 -3.3794966e-04  3.1579842e-03  3.4176917e-03\n",
      "  3.3303302e-06  7.3370477e-04  3.4401997e-03 -3.7586696e-03\n",
      " -4.1556821e-04  3.1623659e-03  1.4756476e-03 -1.5495359e-03\n",
      " -2.7437031e-03  6.4549397e-04 -4.9528144e-03  2.7911407e-03\n",
      "  2.3590417e-06 -3.3605373e-03  4.6052337e-03 -7.4633089e-04\n",
      " -8.5791486e-04 -1.7375943e-03 -7.6174189e-04 -3.6329945e-04\n",
      " -4.2570485e-03  1.2361512e-03  4.1347621e-03 -4.9136899e-04\n",
      "  1.8771290e-03  2.6406622e-03  2.1952996e-03  4.9177646e-03\n",
      " -9.4930659e-04  1.1164770e-03  2.3556787e-03  2.3609604e-05\n",
      " -2.9038878e-03 -6.1664096e-04  2.7539674e-03  3.0503401e-03\n",
      " -1.1646438e-03 -1.2348839e-03  1.6119959e-03  2.0943040e-03\n",
      " -4.1294470e-03  3.9441390e-03  1.0892061e-03  4.8625036e-03]\n"
     ]
    }
   ],
   "source": [
    "#Summarize the loaded model\n",
    "print(model_cbow)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(model_cbow.wv.vocab)\n",
    "print(words)\n",
    "\n",
    "#Acess vector for one word\n",
    "print(model_cbow['dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e37e186-a2da-48d5-8822-dbd702f25b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between eats and bites: -0.0658826\n",
      "Similarity between eats and man: 0.0586519\n"
     ]
    }
   ],
   "source": [
    "#Compute similarity \n",
    "print(\"Similarity between eats and bites:\",model_cbow.similarity('eats', 'bites'))\n",
    "print(\"Similarity between eats and man:\",model_cbow.similarity('eats', 'man'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accd1af4-1420-46ad-83df-adf20bee8751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food', 0.1552993208169937),\n",
       " ('eats', 0.06202014908194542),\n",
       " ('bites', 0.03326358646154404),\n",
       " ('dog', -0.15304674208164215),\n",
       " ('man', -0.15878522396087646)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most similarity\n",
    "model_cbow.most_similar('meat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb4bbb2-2f2f-4fa5-ab9c-36102947b860",
   "metadata": {},
   "source": [
    "### SkipGram\n",
    "SkipGram is very similar to CBOW, with some minor changes. In\n",
    "SkipGram, the task is to predict the context words from the center\n",
    "word.\n",
    "\n",
    "![](images/skipgram.png)\n",
    "<center>SkipGram: given the center word, predict every word in context</center>\n",
    "\n",
    "![](images/cbow-skipgram.png)\n",
    "<center>cbow-skipgram model</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22437c6-db28-443b-bae8-77e57c9f5772",
   "metadata": {},
   "source": [
    "Using packages like gensim, it’s pretty straightforward from a code\n",
    "point of view to implement Word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8692957-9046-4650-a068-9354f7a65daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=6, size=100, alpha=0.025)\n",
      "['dog', 'bites', 'man', 'eats', 'meat', 'food']\n",
      "[-2.2637864e-04  1.1613251e-03 -1.0871805e-03  2.7705024e-03\n",
      " -3.1460954e-03 -1.5414666e-03  1.0732063e-04  2.1121099e-03\n",
      "  1.3804922e-03 -8.9474044e-05 -1.9507032e-04 -3.7300987e-03\n",
      "  1.2606074e-03 -4.3903719e-04  9.7034819e-05 -2.3361982e-03\n",
      "  3.0791026e-03 -7.3196675e-04 -1.5462874e-03  4.1705086e-03\n",
      " -1.2412993e-03  1.2338976e-03 -4.3189838e-03 -1.8837935e-03\n",
      " -4.1378485e-03 -4.2353724e-03 -4.7259713e-03  2.3909356e-03\n",
      " -4.4260966e-03 -6.0296076e-04  5.2839715e-04 -4.4406243e-03\n",
      " -4.7190804e-03  4.7737407e-03  4.9759797e-03 -3.8089291e-03\n",
      "  2.2341565e-03 -2.6227571e-03  1.8058733e-04 -4.9039810e-03\n",
      "  2.0937852e-03  1.7333912e-03 -1.9353403e-03  3.7302757e-03\n",
      " -3.3402010e-03  2.8572769e-03  3.3832456e-03 -4.9053812e-03\n",
      " -2.1320502e-03  1.7676475e-03 -4.2440277e-03 -2.0944851e-03\n",
      " -4.5314971e-03 -3.3794966e-04  3.1579842e-03  3.4176917e-03\n",
      "  3.3303302e-06  7.3370477e-04  3.4401997e-03 -3.7586696e-03\n",
      " -4.1556821e-04  3.1623659e-03  1.4756476e-03 -1.5495359e-03\n",
      " -2.7437031e-03  6.4549397e-04 -4.9528144e-03  2.7911407e-03\n",
      "  2.3590417e-06 -3.3605373e-03  4.6052337e-03 -7.4633089e-04\n",
      " -8.5791486e-04 -1.7375943e-03 -7.6174189e-04 -3.6329945e-04\n",
      " -4.2570485e-03  1.2361512e-03  4.1347621e-03 -4.9136899e-04\n",
      "  1.8771290e-03  2.6406622e-03  2.1952996e-03  4.9177646e-03\n",
      " -9.4930659e-04  1.1164770e-03  2.3556787e-03  2.3609604e-05\n",
      " -2.9038878e-03 -6.1664096e-04  2.7539674e-03  3.0503401e-03\n",
      " -1.1646438e-03 -1.2348839e-03  1.6119959e-03  2.0943040e-03\n",
      " -4.1294470e-03  3.9441390e-03  1.0892061e-03  4.8625036e-03]\n"
     ]
    }
   ],
   "source": [
    "#Summarize the loaded model\n",
    "print(model_skipgram)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(model_skipgram.wv.vocab)\n",
    "print(words)\n",
    "\n",
    "#Acess vector for one word\n",
    "print(model_skipgram['dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7813d5a-0700-4dc1-8f27-940f46687add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=6, size=100, alpha=0.025)\n",
      "['dog', 'bites', 'man', 'eats', 'meat', 'food']\n",
      "[-2.2637864e-04  1.1613251e-03 -1.0871805e-03  2.7705024e-03\n",
      " -3.1460954e-03 -1.5414666e-03  1.0732063e-04  2.1121099e-03\n",
      "  1.3804922e-03 -8.9474044e-05 -1.9507032e-04 -3.7300987e-03\n",
      "  1.2606074e-03 -4.3903719e-04  9.7034819e-05 -2.3361982e-03\n",
      "  3.0791026e-03 -7.3196675e-04 -1.5462874e-03  4.1705086e-03\n",
      " -1.2412993e-03  1.2338976e-03 -4.3189838e-03 -1.8837935e-03\n",
      " -4.1378485e-03 -4.2353724e-03 -4.7259713e-03  2.3909356e-03\n",
      " -4.4260966e-03 -6.0296076e-04  5.2839715e-04 -4.4406243e-03\n",
      " -4.7190804e-03  4.7737407e-03  4.9759797e-03 -3.8089291e-03\n",
      "  2.2341565e-03 -2.6227571e-03  1.8058733e-04 -4.9039810e-03\n",
      "  2.0937852e-03  1.7333912e-03 -1.9353403e-03  3.7302757e-03\n",
      " -3.3402010e-03  2.8572769e-03  3.3832456e-03 -4.9053812e-03\n",
      " -2.1320502e-03  1.7676475e-03 -4.2440277e-03 -2.0944851e-03\n",
      " -4.5314971e-03 -3.3794966e-04  3.1579842e-03  3.4176917e-03\n",
      "  3.3303302e-06  7.3370477e-04  3.4401997e-03 -3.7586696e-03\n",
      " -4.1556821e-04  3.1623659e-03  1.4756476e-03 -1.5495359e-03\n",
      " -2.7437031e-03  6.4549397e-04 -4.9528144e-03  2.7911407e-03\n",
      "  2.3590417e-06 -3.3605373e-03  4.6052337e-03 -7.4633089e-04\n",
      " -8.5791486e-04 -1.7375943e-03 -7.6174189e-04 -3.6329945e-04\n",
      " -4.2570485e-03  1.2361512e-03  4.1347621e-03 -4.9136899e-04\n",
      "  1.8771290e-03  2.6406622e-03  2.1952996e-03  4.9177646e-03\n",
      " -9.4930659e-04  1.1164770e-03  2.3556787e-03  2.3609604e-05\n",
      " -2.9038878e-03 -6.1664096e-04  2.7539674e-03  3.0503401e-03\n",
      " -1.1646438e-03 -1.2348839e-03  1.6119959e-03  2.0943040e-03\n",
      " -4.1294470e-03  3.9441390e-03  1.0892061e-03  4.8625036e-03]\n"
     ]
    }
   ],
   "source": [
    "#Summarize the loaded model\n",
    "print(model_skipgram)\n",
    "\n",
    "#Summarize vocabulary\n",
    "words = list(model_skipgram.wv.vocab)\n",
    "print(words)\n",
    "\n",
    "#Acess vector for one word\n",
    "print(model_skipgram['dog'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e28cc1-5c5d-4e15-8e8d-7bc652d9fd18",
   "metadata": {},
   "source": [
    "# 4. Distributed Representations Beyond Words and Characters\n",
    "Let’s look at another approach, Doc2vec, which allows us to directly\n",
    "learn the representations for texts of arbitrary lengths (phrases,\n",
    "sentences, paragraphs, and documents) by taking the context of words\n",
    "in the text into account.\n",
    "\n",
    "The\n",
    "two architectures are called *distributed memory (DM)* and *distributed\n",
    "bag of words (DBOW)*.\n",
    "\n",
    "![](images/dm.png)\n",
    "<center>Doc2vec architectures: DM (left) and DBOW (right)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ced55fb-d5d8-4a8f-9359-fef0804c54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import spacy and load the model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") #here nlp object refers to the 'en_core_web_sm' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddcae6d2-bd1b-4b20-b120-7b2dad9b1264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document After Pre-Processing: ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n",
      "------------------------------\n",
      "Average Vector of 'dog bites man'\n",
      " [ 0.35036066  0.10273071  0.33009622 -0.2030462   0.397346   -0.05984474\n",
      " -0.2201394   0.25512496 -0.3575406   0.39600918 -0.75429106 -0.5888314\n",
      "  0.29484284 -0.63774514 -0.23350935  0.5900816  -0.2566284  -0.71845955\n",
      "  0.20040572  0.7679166  -0.26526675 -0.6816276  -0.0701522   0.04820635\n",
      "  0.1266749   0.2589217  -0.6932214  -0.3419633   1.0904325  -0.32465276\n",
      "  1.4362421  -0.5931116   0.32251295 -0.341225   -0.12486354 -0.7798831\n",
      " -0.29717746  0.4014299  -0.1318171   0.910722   -0.41182932  0.04191664\n",
      "  0.59365046 -0.04422406 -0.18440922 -0.05003772  0.59136873 -0.6386824\n",
      "  1.8019737  -0.04936111  0.27116123  0.21994926 -0.2368415  -0.23461938\n",
      "  0.22323321  1.0983983  -0.39096567  0.10752393 -0.06386908  0.14312072\n",
      "  0.37180772 -0.34773377 -0.42992604 -0.4652144  -0.58004665  0.37198398\n",
      "  0.04235339 -0.4719428   0.28124237 -1.1937796  -0.13171527 -0.5805836\n",
      " -0.19396889 -0.28201875 -0.208987   -0.23234288 -0.23534824 -0.21468079\n",
      "  0.22775708 -0.49260768  0.6319783  -0.33251083  0.20568585  0.35996032\n",
      "  0.2478166  -0.03568138  0.6632246  -0.9730921  -0.70129174 -0.15219457\n",
      " -0.20672785 -0.47586355  0.02362418  0.31522077 -0.05710061  0.10075549]\n",
      "\n",
      "dog [ 1.0456305   0.43316543  0.919306   -1.0345336   0.76348233 -0.00848687\n",
      " -0.39562935  0.53843355  0.10359538 -0.29383948 -0.31168646 -1.2970233\n",
      "  0.06805409 -0.49767178  0.32073534  0.2348006  -0.61145496 -0.00401674\n",
      "  0.8336798   2.2356164   0.04831797 -1.2297006  -0.03746891  1.2826033\n",
      " -0.13337824 -0.66164255 -0.517896   -0.35937893  0.6922027  -0.23472422\n",
      "  1.5772134  -0.12950069  0.26141202 -0.28020307 -0.27356595 -0.3352664\n",
      " -0.87879235  1.0150697   0.09796268  1.2844213  -1.0784308   0.26590428\n",
      "  0.42636067 -0.34792465 -0.6371386   0.50521016  0.37601754 -0.5935571\n",
      "  2.2693224  -0.35263935  0.47877675  0.16542971 -1.2613671  -0.5979384\n",
      "  0.3128377   1.2138872  -0.6509563  -0.18787587 -0.11951631  0.5417869\n",
      " -0.5704644  -0.9797213  -0.67144537 -0.6551513  -0.72967875 -0.07893744\n",
      "  0.6895182  -0.4443028   0.14369014 -1.5259092  -0.37458307 -0.19363947\n",
      "  0.4914209  -0.5420234  -0.06658001 -1.212701   -0.02432311 -1.2058704\n",
      "  0.05359304 -1.0201803   1.1003057   0.04556808  0.14692572  0.7253051\n",
      " -0.28571367  0.19904527  0.23330429 -0.5213322  -1.0205765  -0.39977813\n",
      " -1.0537221  -0.23744887  0.08160639  1.66261     0.3657593   0.4806698 ]\n",
      "\n",
      "bites [ 5.3912535e-02  7.5856572e-01  5.5808508e-01  3.7149405e-01\n",
      "  5.6598222e-01  2.4550590e-01 -4.8027506e-01  5.0831223e-01\n",
      " -6.5079927e-01  1.7801464e-01 -6.9857311e-01 -2.5449967e-01\n",
      " -9.0923473e-02  8.6104974e-02 -4.3143460e-01  7.6735681e-01\n",
      " -3.8136649e-01 -8.6830449e-01 -7.5469196e-01 -9.5328093e-03\n",
      " -8.3344471e-01 -1.7345819e-01  7.0297450e-01 -1.0729223e+00\n",
      "  5.0658453e-01  1.0055926e+00 -3.3366445e-01  6.7706555e-02\n",
      "  1.5615284e+00  2.8820789e-01  8.0305624e-01 -1.2786309e-01\n",
      "  6.6421598e-02 -4.0417430e-01 -9.2036158e-02 -1.0209525e+00\n",
      "  5.2115238e-01 -3.9504468e-02  2.3798333e-01  5.6574345e-02\n",
      "  5.4148191e-01  1.2780876e-01  3.0502263e-01 -9.7296351e-01\n",
      " -2.7481608e-02 -1.0910573e+00  9.6510035e-01 -2.3121053e-01\n",
      "  7.8075510e-01 -4.9973482e-01  5.4688685e-02 -6.4976627e-01\n",
      "  2.0858756e-01 -4.2240331e-01  1.6771141e-01  1.4045311e+00\n",
      " -6.0216469e-01  1.7977452e-01 -6.4457566e-02  5.1466095e-01\n",
      "  1.4829495e+00  2.6155978e-01 -1.1220368e+00  7.0380911e-02\n",
      " -1.2726679e+00  4.2076734e-01 -4.3465543e-01 -8.1329167e-01\n",
      "  5.1158214e-01 -1.3313344e+00 -2.9227832e-01 -1.0123371e+00\n",
      " -6.7134464e-01  2.0081043e-02 -1.0057882e+00  2.0708592e-01\n",
      " -3.7196767e-01 -1.1900806e+00  8.3494365e-01  5.0711673e-01\n",
      " -4.7964638e-01 -1.1339858e-02  4.1638529e-01  2.3711373e-01\n",
      "  5.9969544e-01  1.7562598e-02  1.2356110e+00 -1.0312194e+00\n",
      " -2.6960611e-01 -8.0063939e-04  7.9333490e-01 -6.5455347e-01\n",
      "  5.7081831e-01 -9.5358163e-01 -7.8930640e-01  4.6121377e-01]\n",
      "\n",
      "man [-0.0484609  -0.8835391  -0.48710233  0.05390096 -0.13742666 -0.41655326\n",
      "  0.21548623 -0.28137088 -0.5254179   1.3038523  -1.2526137  -0.21497136\n",
      "  0.90739787 -1.5016687  -0.5898288   0.76808727  0.2229363  -1.2830573\n",
      "  0.5222293   0.07766613 -0.01067355 -0.641724   -0.8759622  -0.06506187\n",
      "  0.00681841  0.4328151  -1.2281036  -0.7342175   1.0175662  -1.027442\n",
      "  1.9284565  -1.521971    0.6397052  -0.33929765 -0.0089885  -0.9834305\n",
      " -0.5338924   0.22872448 -0.73139733  1.3911704  -0.698539   -0.26796314\n",
      "  1.049568    1.188216    0.11139259  0.43573397  0.43298832 -1.0912797\n",
      "  2.3558433   0.70429087  0.28001824  1.1441844   0.34225506  0.3164836\n",
      "  0.18915051  0.67677677  0.08022389  0.33067313 -0.00763336 -0.6270857\n",
      "  0.20293808 -0.32503983  0.5037041  -0.8108729   0.26220655  0.774122\n",
      " -0.12780261 -0.15823396  0.18845487 -0.7240951   0.2717156  -0.5357742\n",
      " -0.40198293 -0.32411385  0.4454073   0.3085864  -0.30975395  1.7519087\n",
      " -0.20526546 -0.96475935  1.2752755  -1.0317607   0.05374655  0.11746222\n",
      "  0.42946804 -0.323652    0.5207585  -1.3667248  -0.8136927  -0.05600496\n",
      " -0.3597963  -0.5355882  -0.58155215  0.23663388  0.25224528 -0.6396171 ]\n",
      "------------------------------\n",
      "Average Vector of 'man bites dog'\n",
      " [ 0.69313866  0.22606027  0.21485941 -0.13014205 -0.04740261 -0.3634933\n",
      " -0.612554    0.33093917 -0.5719232   0.22212799 -0.7501278  -0.8262256\n",
      "  0.45667818 -0.23508793 -0.32394844  0.9326987  -0.5549075   0.21069913\n",
      "  0.19657613  0.225613   -0.40230998 -0.7504983  -0.1536261  -0.01891213\n",
      "  0.281518   -0.028749   -0.5527249  -0.23994415  1.2803789  -0.22852284\n",
      "  0.99096173 -0.84199625  0.2710339  -0.03586351  0.29288253 -0.72613114\n",
      " -0.10758     0.6488024   0.03140668  1.0002292  -0.24966908 -0.3271272\n",
      "  0.4479085   0.14901742 -0.4883041  -0.3500476   0.5775156  -0.33246133\n",
      "  1.9340502   0.07172325  0.320449    0.54487973 -0.1390981  -0.27020445\n",
      "  0.00954175  0.9388354  -0.42501983  0.310541   -0.10272732  0.31442508\n",
      "  0.12220053 -0.2813793  -0.5302014  -0.4145145  -0.4514304   0.47410986\n",
      " -0.39565918 -0.0999169   0.492502   -1.243157   -0.03042855 -0.70453316\n",
      " -0.3198954  -0.1095263  -0.1792277  -0.11021443 -0.2976153   0.46326447\n",
      "  0.6179961  -0.5886492   0.49013913 -0.2510657   0.38385925 -0.01534534\n",
      " -0.15616919 -0.01071994  0.5808062  -0.5346918  -0.6044248  -0.40863279\n",
      " -0.5935361  -0.58394563 -0.22761677  0.46137115 -0.03751655  0.5770183 ]\n",
      "\n",
      "man [ 1.6164255   0.9321022   0.46761945 -1.0039053  -0.08127132  0.19127497\n",
      " -0.72247696  0.39024457 -0.20011401 -0.14924136 -0.6633725  -0.9373424\n",
      "  0.34054577 -0.3482175  -0.8189194   1.3256158  -0.68953925 -0.15151355\n",
      "  0.39220953  1.2534717  -0.17383838 -1.0105928  -0.14844134  1.0070454\n",
      " -0.02606392 -0.638499   -0.7362757   0.03868914  1.0099933  -0.50839216\n",
      "  2.042929   -0.49273935 -0.3084139   0.0696283   0.3104994  -1.5847361\n",
      " -1.022844    1.1255076  -0.06162077  1.0227274  -1.3610045   0.41782397\n",
      "  0.9361388  -0.34610695 -0.63489604  0.17169255  0.44388524 -0.60685384\n",
      "  1.6173348   0.62502897  0.60166407  0.16660555 -0.40422124 -0.39137343\n",
      " -0.33486786  0.5181363  -0.31046718  0.05918041 -0.9557043   0.47684526\n",
      "  0.4349867  -0.30728558 -0.42638767  0.13663149 -0.71834147  1.1068854\n",
      " -0.44112632  0.92542136  0.9479183  -1.370817   -0.10707232 -0.78630996\n",
      "  0.2746983  -0.70067203 -0.34977445 -1.0137349   0.18752366 -0.89903414\n",
      "  0.58035475 -0.35314205  0.20675829 -0.10559238  0.43428364  0.7920559\n",
      " -0.23334174 -0.01646236  0.13394375  0.20952234 -0.5865743  -0.6180528\n",
      " -1.4731077  -0.48263103 -0.00904247  1.6508043   0.11023232  0.9236099 ]\n",
      "\n",
      "bites [ 1.1622839  -0.14216542  0.26241416  0.426169    0.06053725 -0.7641785\n",
      " -0.6283173   0.8698739  -0.92250025  0.08246911 -0.53963315 -0.81538224\n",
      "  0.31107545  0.3271733   0.10568553  0.54567903 -0.7409977   0.4801218\n",
      " -0.24669793 -0.5817495  -0.9754127  -0.23003894  0.1780433  -1.3404548\n",
      "  0.83639055  0.5205215   0.16507867 -0.03839348  2.2570736   0.5000948\n",
      " -0.43319803 -0.4196078  -0.39331016 -0.03587866  0.7965324   0.26960358\n",
      "  0.8095884   0.7547      0.29034972  0.02168563  1.2322309  -0.6291357\n",
      "  0.12799495 -0.03013995 -0.61604697 -0.8287517   1.0353966   0.22573861\n",
      "  1.7282989  -0.57263005  0.37099817 -0.33911818  0.0038859  -0.4665175\n",
      " -0.05464818  1.4091365  -0.4040562  -0.06539202  0.6053202   0.82306886\n",
      "  0.94974     0.29973128 -1.6721597  -0.08128437 -0.9864197   0.42680353\n",
      " -0.53319013 -0.2574745   0.26992863 -1.3086509  -0.34822497 -0.8464843\n",
      " -1.4950662   0.07780689 -0.877834    0.06126488 -0.62982506  0.42535448\n",
      "  0.634173   -0.03272235 -0.42445928 -0.72299194  0.4290136  -0.25818083\n",
      " -0.29949617  0.25776398  0.9023305  -1.0857241  -0.7265322  -0.53300023\n",
      "  0.37073666 -0.97749996  0.28815725 -0.6602802  -0.65555596  1.2952486 ]\n",
      "\n",
      "dog [-0.6992933  -0.11175597 -0.0854554   0.18731011 -0.12147377 -0.5175763\n",
      " -0.48686764 -0.26730093 -0.5931554   0.7331562  -1.0473776  -0.72595227\n",
      "  0.71841335 -0.6842196  -0.25861144  0.9268015  -0.23418552  0.30348915\n",
      "  0.44421682  0.00511679 -0.05767882 -1.0108632  -0.49048027  0.27667305\n",
      "  0.03422743  0.0317305  -1.0869777  -0.72012806  0.57407    -0.6772711\n",
      "  1.3631542  -1.6136417   1.5148258  -0.14134018 -0.2283842  -0.86326075\n",
      " -0.1094844   0.06619969 -0.13450891  1.9562745  -0.62023365 -0.77006984\n",
      "  0.27959174  0.82329917 -0.21396938 -0.39308366  0.253265   -0.61626875\n",
      "  2.4565172   0.16277081 -0.01131526  1.8071518  -0.01695896  0.04727754\n",
      "  0.4181413   0.88923335 -0.560536    0.9378346   0.04220209 -0.35663888\n",
      " -1.0181252  -0.8365837   0.5079434  -1.2988906   0.35046998 -0.11135951\n",
      " -0.21266103 -0.9676975   0.2596592  -1.0500036   0.36401165 -0.4808052\n",
      "  0.26068178  0.29428625  0.6899254   0.62182677 -0.45054448  1.863473\n",
      "  0.63946056 -1.3800832   1.6881183   0.07538718  0.2882805  -0.5799111\n",
      "  0.06433034 -0.27346146  0.7061442  -0.7278736  -0.5001677  -0.07484543\n",
      " -0.6782373  -0.29170597 -0.9619651   0.39358938  0.43277395 -0.4878035 ]\n",
      "------------------------------\n",
      "Average Vector of 'dog eats meat'\n",
      " [-1.70424104e-01  9.09564197e-01  2.72495151e-01 -1.20343566e-01\n",
      "  8.90528202e-01 -9.51907560e-02 -6.21061385e-01 -1.07506551e-01\n",
      " -2.13841721e-01 -2.34627768e-01 -6.28074944e-01 -3.89072508e-01\n",
      "  3.90962452e-01 -5.98861039e-01  1.40387863e-02  2.69287556e-01\n",
      " -2.61953592e-01 -4.29675579e-01 -5.76387346e-03 -7.81933486e-04\n",
      "  3.17344069e-03 -4.28394705e-01  1.37667492e-01  1.33815408e-01\n",
      " -2.54129052e-01  1.68412700e-01 -8.60214055e-01 -5.08502781e-01\n",
      "  2.93680847e-01 -3.92385572e-01  6.16225421e-01 -3.01621079e-01\n",
      " -1.12342238e-02 -3.19477677e-01 -1.70976564e-01 -1.06324756e+00\n",
      " -5.79883814e-01 -8.73152837e-02  5.96220970e-01  8.97610843e-01\n",
      "  4.38372493e-02 -3.83674175e-01  3.94693822e-01 -1.37257576e-01\n",
      " -2.39031956e-01 -3.71304065e-01  2.34758973e-01 -2.39034310e-01\n",
      "  7.53699362e-01 -2.61850089e-01 -2.18898311e-01  1.36073783e-01\n",
      " -2.19922200e-01 -4.56972532e-02 -4.01326507e-01  1.15978491e+00\n",
      " -4.13318306e-01  3.14154536e-01 -1.25746325e-01 -3.74039054e-01\n",
      "  8.49580228e-01 -3.52116935e-02  3.93941812e-02 -4.40231174e-01\n",
      " -5.13821542e-01 -1.18280351e-02 -3.00248086e-01 -6.64656937e-01\n",
      "  2.39028677e-01 -3.09504002e-01 -1.55247912e-01 -5.57870716e-02\n",
      "  2.05314577e-01  3.92904282e-01 -3.80266309e-02  1.57820687e-01\n",
      " -3.57231766e-01  6.53909683e-01  3.78273040e-01 -1.05747426e+00\n",
      "  4.75616723e-01  1.10179216e-01 -4.72838134e-01  6.38946474e-01\n",
      "  1.75086305e-01 -8.17900524e-02  5.91906071e-01 -5.01064360e-01\n",
      " -3.49890023e-01  1.62043381e+00 -7.18620777e-01  2.09273994e-01\n",
      "  1.55843511e-01  5.33672035e-01  1.31984549e-02 -1.64448604e-01]\n",
      "\n",
      "dog [ 0.47848696  0.78268623  0.9074937  -0.6238762   0.30113608  0.7276031\n",
      " -0.3981342   0.1428417   0.48130962 -0.40293172 -0.6970245  -1.2221444\n",
      "  0.01705006 -0.660798    0.15429889 -0.34814167 -0.76402885  0.32767415\n",
      "  0.55420333  1.3594018   0.24810484 -1.0722406   1.3873351  -0.07784441\n",
      " -0.57505906 -0.7248632  -0.15505975 -0.4583429   0.18411352 -0.20409757\n",
      "  1.8441225  -0.55200654  0.3389226   0.00900558 -0.3222775  -1.5499556\n",
      " -1.01665     0.4091733  -0.272497    1.107618   -0.8300797   0.2355929\n",
      "  0.59991246 -0.29470992 -0.781538   -0.5723277  -0.0640029  -0.14239517\n",
      "  2.3791296  -0.04281056  0.27255195  0.6186986  -1.1415808  -0.02588233\n",
      "  0.3277983   1.3120383  -1.0672976  -0.04044393 -0.08383696 -0.35977933\n",
      "  0.35796264 -0.8171408   0.7604536  -0.7535527  -0.7596415  -0.8758273\n",
      "  0.21871655 -0.70791066  0.28274137 -0.76274645  0.31954652  0.01551588\n",
      "  2.2386892  -0.7019563  -0.10378203 -1.1805724  -0.56484044  0.34123075\n",
      " -0.26695845 -1.5493933   1.4542457   0.8081532   0.00737964  1.0929598\n",
      " -0.45049924  0.36705348  0.4725438  -0.7249311  -0.6812862   0.5364922\n",
      " -1.2546841   0.3148194   0.13326967  1.0271404   0.03621492 -0.4376477 ]\n",
      "\n",
      "eats [-2.2919986e-01  4.7065520e-01  5.9212941e-01  4.9471417e-01\n",
      "  1.4529406e+00 -6.0058534e-01 -6.3015640e-01  4.5091823e-01\n",
      " -5.0919664e-01 -1.3941228e-01 -3.3364010e-01  4.6829766e-01\n",
      "  4.7129351e-01 -3.7195891e-01 -2.4423897e-03  5.9529519e-01\n",
      " -4.8126394e-01 -1.4202278e+00 -7.0609379e-01 -5.5213839e-01\n",
      " -5.1313818e-01 -2.6501000e-02  4.1654134e-01 -1.1881596e+00\n",
      " -4.7929588e-01  8.0742013e-01 -8.2101053e-01 -6.7035180e-01\n",
      " -4.1968644e-02 -1.4585617e-01 -7.6485342e-01 -4.0325344e-01\n",
      " -8.6563820e-01 -3.8031739e-01 -8.4415090e-01 -1.1555985e+00\n",
      "  2.8916746e-03 -2.0372987e-01  2.9464982e+00  7.9434657e-01\n",
      "  5.8141875e-01 -6.6130430e-02  6.7242867e-01 -7.0058376e-01\n",
      "  1.9006824e-01 -4.3024844e-01  4.5876256e-01 -4.0029982e-01\n",
      " -8.8673496e-01 -3.8241702e-01 -8.6972833e-01 -8.9649081e-01\n",
      "  5.7186490e-01 -5.2659184e-01 -1.0819405e+00  1.1996988e+00\n",
      " -4.7064048e-01  1.5468419e-02 -3.9789921e-01  1.4648256e-01\n",
      "  1.8327547e+00  2.1193993e-01 -9.2233419e-01  2.5072151e-01\n",
      " -1.0886886e+00  5.9672225e-01 -1.4354161e+00 -3.4303465e-01\n",
      "  4.4322464e-01 -2.2294804e-01 -5.3164989e-01 -1.8409505e-01\n",
      " -1.4356003e+00  1.2787354e+00  4.6581268e-02  8.8255113e-01\n",
      " -3.1041497e-01  7.8191221e-02  7.1781844e-01  1.6043216e-01\n",
      " -4.2383057e-01 -5.4984033e-01 -2.8090143e-01  5.3937477e-01\n",
      "  6.8501335e-01  1.9575866e-01  1.3183379e+00 -3.9119026e-01\n",
      "  3.0410385e-01  4.1582680e+00  5.1302850e-01  1.2681788e-01\n",
      "  1.7139435e-01 -5.9022623e-01 -8.2987040e-01  2.4645472e-01]\n",
      "\n",
      "meat [-7.60559380e-01  1.47535110e+00 -6.82137609e-01 -2.31868640e-01\n",
      "  9.17507887e-01 -4.12589997e-01 -8.34893644e-01 -9.16279554e-01\n",
      " -6.13638163e-01 -1.61539331e-01 -8.53560209e-01 -4.13370758e-01\n",
      "  6.84543729e-01 -7.63826191e-01 -1.09740138e-01  5.60709119e-01\n",
      "  4.59432006e-01 -1.96473151e-01  1.34598836e-01 -8.09609234e-01\n",
      "  2.74553657e-01 -1.86442554e-01 -1.39087391e+00  1.66745019e+00\n",
      "  2.91967750e-01  4.22681153e-01 -1.60457194e+00 -3.96813691e-01\n",
      "  7.38897681e-01 -8.27202916e-01  7.69406974e-01  5.03967702e-02\n",
      "  4.93012965e-01 -5.87121189e-01  6.53498650e-01 -4.84188676e-01\n",
      " -7.25893140e-01 -4.67389286e-01 -8.85338306e-01  7.90867925e-01\n",
      "  3.80172670e-01 -1.32048500e+00 -8.82597417e-02  5.83520949e-01\n",
      " -1.25626057e-01 -1.11336134e-01  3.09517294e-01 -1.74407974e-01\n",
      "  7.68703461e-01 -3.60322654e-01 -5.95185831e-02  6.86013579e-01\n",
      " -9.00507048e-02  4.15382385e-01 -4.49837297e-01  9.67617452e-01\n",
      "  2.97983170e-01  9.67439175e-01  1.04497209e-01 -9.08820391e-01\n",
      "  3.58023107e-01  4.99565810e-01  2.80063152e-01 -8.17862332e-01\n",
      "  3.06865513e-01  2.43620962e-01  3.15955341e-01 -9.43025470e-01\n",
      " -8.87995958e-03  5.71825206e-02 -2.53640354e-01  1.21796131e-03\n",
      " -1.87145174e-01  6.01933718e-01 -5.68791330e-02  7.71483302e-01\n",
      " -1.96439862e-01  1.54230702e+00  6.83959126e-01 -1.78346157e+00\n",
      "  3.96435142e-01  7.22247735e-02 -1.14499259e+00  2.84505010e-01\n",
      "  2.90744752e-01 -8.08182299e-01 -1.51635408e-02 -3.87071729e-01\n",
      " -6.72487736e-01  1.66541159e-01 -1.41420674e+00  1.86184704e-01\n",
      "  1.62866503e-01  1.16410184e+00  8.33250880e-01 -3.02152812e-01]\n",
      "------------------------------\n",
      "Average Vector of 'man eats food'\n",
      " [-0.03966542  0.8069971  -0.06643438 -0.24360573  0.76492053 -0.2676132\n",
      " -0.51112187 -0.10503217 -0.29475775 -0.4293352  -0.7569022  -0.37408385\n",
      "  0.3547813  -0.23451042 -0.32934377  0.27832595 -0.29179436 -0.44370136\n",
      " -0.07511113 -0.6068233  -0.04473857 -0.3307257   0.306064    0.28225997\n",
      " -0.02278286 -0.1280223  -0.8147528  -0.48048234  0.5095705  -0.4586937\n",
      "  0.39026418 -0.7279379   0.00931251 -0.235201   -0.02183292 -0.8347283\n",
      " -0.5190598  -0.08447725  0.6256334   0.98645353  0.14192605 -0.38317797\n",
      "  0.5822787  -0.20966892 -0.3763992  -0.54545254 -0.12669088 -0.27415344\n",
      "  0.82862073  0.05398881 -0.5730805   0.6311756  -0.22344142  0.08145399\n",
      " -0.59848756  1.3480567  -0.36196446  0.45537892 -0.22657263 -0.6070251\n",
      "  1.0755701  -0.07149849  0.27984515 -0.3455795  -0.19994415  0.17048615\n",
      " -0.41708887 -0.3553191   0.4396627  -0.3538474   0.00618841  0.00797068\n",
      "  0.2423207   0.60912657  0.03929846 -0.13768227 -0.18078001  0.9985261\n",
      "  0.13965781 -0.80527335  0.60213333 -0.2159432  -0.15874732  0.75061035\n",
      "  0.06405687  0.01626416  0.6465297  -0.3681356  -0.5912617   1.4392456\n",
      " -0.73437357  0.44007316 -0.21960156  0.70838857 -0.04939075 -0.00888047]\n",
      "\n",
      "man [ 0.3996513   1.0713799   0.43886244 -1.2235435   0.13385251  0.67294836\n",
      " -0.2512303  -0.3141303   0.16781358 -0.78923315 -0.96570265 -0.5686592\n",
      "  0.11408202 -0.7107735  -1.0094976  -0.32356513 -0.21776584 -0.18079838\n",
      "  0.57504964  0.45832545 -0.19209956 -0.7984246   1.1874894  -0.05322717\n",
      " -0.48895168 -1.1574743  -0.22017671 -0.07182799  0.4313498  -0.31863374\n",
      "  2.049631   -0.8054915  -0.0737744   0.06917036  0.14034815 -1.3131655\n",
      " -1.3001995   0.26889223 -0.7642328   0.71443677 -0.66633296  0.4881886\n",
      "  1.297807   -0.63576996 -1.0498999  -0.6569213  -0.26490504 -0.12007514\n",
      "  2.4992192   0.568993    0.0461598   0.9842393  -0.63722044  0.35693344\n",
      " -0.4688038   1.1202571  -0.6736821  -0.45996538 -0.7244915  -0.71123147\n",
      "  1.4260622  -0.49845773  0.85935885 -0.03856051 -0.42370546  0.26464832\n",
      " -0.5099192   0.22190775  0.8300437  -0.7107127   0.66210186  0.40499496\n",
      "  1.9111037  -0.67105746 -0.37048832 -1.2341692  -0.38205242  0.6432332\n",
      " -0.17863034 -0.2180151   0.7481249   0.17983133  0.8898583   1.4612168\n",
      " -0.37683037  0.29151887  0.76286083 -0.32855916 -1.2223662   0.25693896\n",
      " -1.077228    0.32895344 -0.0599863   1.11656    -0.13763124 -0.63028634]\n",
      "\n",
      "eats [ 0.27003983  0.11233582  0.17938757  0.673152    0.9314734  -0.6243869\n",
      " -0.5843887   0.7374326  -0.55221254 -0.7703413  -0.31092462  0.27782202\n",
      "  0.07460934 -0.37295353  0.16243288  0.30486548 -0.53190976 -1.1579225\n",
      " -0.581744   -0.9667416  -0.8092369  -0.10797569  0.5825074  -0.9774354\n",
      " -0.40259507  0.646363   -0.33466977 -0.86150885  0.09012452 -0.16448326\n",
      " -0.91971767 -0.49437624 -0.8397994  -0.03462818 -0.6147874  -1.030018\n",
      "  0.03764908  0.1049435   2.8945353   0.6340087   0.54061514 -0.36971432\n",
      "  0.3460238  -0.6101152   0.05957741 -0.20574576  0.06505308 -0.34240365\n",
      " -0.97250473 -0.15664232 -1.0996004  -0.43663856  0.3858096  -0.43108374\n",
      " -1.4224321   1.2597164  -0.1691525   0.06029531 -0.26135445  0.40177876\n",
      "  1.53241     0.46231622 -0.77183807  0.04349041 -0.6254817   0.09162933\n",
      " -0.9471917   0.00460374  0.5623316  -0.41223872 -1.1789118   0.09136102\n",
      " -1.3858345   1.5127382   0.51225954  0.5494444  -0.20416617  0.65499127\n",
      "  0.6692978  -0.32729855 -0.14674029 -0.72037184 -0.3526097   0.90155464\n",
      "  0.25640798  0.19629255  1.46877    -0.5880872  -0.03153457  4.3955736\n",
      "  0.11069115  0.35003936  0.37123343 -0.24089976 -0.8759588   0.85560685]\n",
      "\n",
      "food [-0.78868735  1.2372756  -0.81755316 -0.18042573  1.2294357  -0.8514011\n",
      " -0.69774663 -0.7383988  -0.4998743   0.2715688  -0.99407923 -0.83141434\n",
      "  0.87565255  0.38019583 -0.14096656  0.85367745 -0.12570745  0.00761682\n",
      " -0.21863902 -1.3120538   0.86712074 -0.08577673 -0.85180473  1.8774425\n",
      "  0.82319814  0.12704435 -1.889412   -0.5081102   1.0072371  -0.8929641\n",
      "  0.04087907 -0.88394576  0.94151133 -0.7401452   0.4089405  -0.1610015\n",
      " -0.2946289  -0.6272675  -0.25340217  1.6109151   0.55149597 -1.2680081\n",
      "  0.10300545  0.6168784  -0.13887508 -0.7736904  -0.18022066 -0.35998148\n",
      "  0.9591479  -0.2503842  -0.6658009   1.345926   -0.41891342  0.31851226\n",
      "  0.09577321  1.6641966  -0.24305886  1.7658069   0.30612803 -1.5116224\n",
      "  0.268238   -0.17835397  0.75201464 -1.0416684   0.4493547   0.15518081\n",
      "  0.20584425 -1.2924688  -0.07338721  0.06140909  0.5353752  -0.47244394\n",
      "  0.20169279  0.985699   -0.02387586  0.27167803  0.04387856  1.6973538\n",
      " -0.07169405 -1.8705065   1.2050154  -0.10728908 -1.0134906  -0.11094046\n",
      "  0.31259298 -0.43901893 -0.29204172 -0.18776043 -0.5198842  -0.33477613\n",
      " -1.2365838   0.64122677 -0.97005177  1.2495055   0.86541784 -0.25196192]\n"
     ]
    }
   ],
   "source": [
    "#Assume each sentence in documents corresponds to a separate document.\n",
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n",
    "processed_docs\n",
    "\n",
    "print(\"Document After Pre-Processing:\",processed_docs)\n",
    "\n",
    "\n",
    "#Iterate over each document and initiate an nlp instance.\n",
    "for doc in processed_docs:\n",
    "    doc_nlp = nlp(doc) #creating a spacy \"Doc\" object which is a container for accessing linguistic annotations. \n",
    "    \n",
    "    print(\"-\"*30)\n",
    "    print(\"Average Vector of '{}'\\n\".format(doc),doc_nlp.vector)#this gives the average vector of each document\n",
    "    for token in doc_nlp:\n",
    "        print()\n",
    "        print(token.text,token.vector)#this gives the text of each word in the doc and their respective vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ef082-b890-48f4-8f8b-67687e13b5d9",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to train a doc2vec model on a custom corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05b85974-f34d-4fa7-9c30-22b5dd628fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\minhh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1865c84b-48f0-4710-82c1-f68dae740b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['dog', 'bites', 'man'], tags=['0']),\n",
       " TaggedDocument(words=['man', 'bites', 'dog'], tags=['1']),\n",
       " TaggedDocument(words=['dog', 'eats', 'meat'], tags=['2']),\n",
       " TaggedDocument(words=['man', 'eats', 'food'], tags=['3'])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"dog bites man\",\n",
    "        \"man bites dog\",\n",
    "        \"dog eats meat\",\n",
    "        \"man eats food\"]\n",
    "\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(word.lower()), tags=[str(i)]) for i, word in enumerate(data)]\n",
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aad7f8ce-0a3f-4c73-94a8-871302e70508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbow\n",
    "model_dbow = Doc2Vec(tagged_data,vector_size=20, min_count=1, epochs=2,dm=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9cf3c1f-059e-4547-9e6a-6e93abe98ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00420315  0.00653283 -0.00940372  0.01937548 -0.02182949 -0.01730926\n",
      "  0.01996748  0.00888949 -0.02249881 -0.0033143   0.014049    0.01591324\n",
      " -0.00174675  0.02163521  0.01903928 -0.0141784   0.0058233  -0.01405842\n",
      "  0.01193869  0.01563022]\n"
     ]
    }
   ],
   "source": [
    "print(model_dbow.infer_vector(['man','eats','food']))#feature vector of man eats food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "861dc8a2-3639-4214-b9ca-234b4beeb46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bites', 0.10207571089267731),\n",
       " ('eats', 0.10173594951629639),\n",
       " ('food', -0.06450934708118439),\n",
       " ('dog', -0.19907894730567932),\n",
       " ('meat', -0.2696749269962311)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.wv.most_similar(\"man\",topn=5)#top 5 most simlar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee31fb73-2607-4737-af81-76be7239ba8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.19907895"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dbow.wv.n_similarity([\"dog\"],[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7db47f8e-c685-4ac9-8530-e15fb0b19477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Vector of man eats food\n",
      "  [ 0.00417791  0.00643509 -0.00935274  0.01932913 -0.02187857 -0.01716197\n",
      "  0.02004039  0.00903782 -0.02242476 -0.00328896  0.01409931  0.01591488\n",
      " -0.00172501  0.02172272  0.01902425 -0.01420487  0.00586274 -0.01404743\n",
      "  0.01185267  0.01552101]\n",
      "Most similar words to man in our corpus\n",
      " [('bites', 0.10207571089267731), ('eats', 0.10173594951629639), ('food', -0.06450934708118439), ('dog', -0.19907894730567932), ('meat', -0.2696749269962311)]\n",
      "Similarity between man and dog:  -0.19907895\n"
     ]
    }
   ],
   "source": [
    "#dm\n",
    "model_dm = Doc2Vec(tagged_data, min_count=1, vector_size=20, epochs=2,dm=1)\n",
    "\n",
    "print(\"Inference Vector of man eats food\\n \",model_dm.infer_vector(['man','eats','food']))\n",
    "\n",
    "print(\"Most similar words to man in our corpus\\n\",model_dm.wv.most_similar(\"man\",topn=5))\n",
    "print(\"Similarity between man and dog: \",model_dm.wv.n_similarity([\"dog\"],[\"man\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9a82f-17f6-404b-9d50-899ca1ac5da9",
   "metadata": {},
   "source": [
    "What happens when we compare between words which are not in the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19c0569c-d737-417b-a790-8840329f61bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'covid' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel_dm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcovid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\minhh\\documents\\dev\\nlp\\.venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1012\u001b[0m, in \u001b[0;36mWordEmbeddingsKeyedVectors.n_similarity\u001b[1;34m(self, ws1, ws2)\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(ws1) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ws2)):\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAt least one of the passed list is empty.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1012\u001b[0m v1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m ws1]\n\u001b[0;32m   1013\u001b[0m v2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m ws2]\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dot(matutils\u001b[38;5;241m.\u001b[39munitvec(array(v1)\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)), matutils\u001b[38;5;241m.\u001b[39munitvec(array(v2)\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))\n",
      "File \u001b[1;32mc:\\users\\minhh\\documents\\dev\\nlp\\.venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:1012\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(ws1) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ws2)):\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAt least one of the passed list is empty.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1012\u001b[0m v1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m ws1]\n\u001b[0;32m   1013\u001b[0m v2 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m ws2]\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dot(matutils\u001b[38;5;241m.\u001b[39munitvec(array(v1)\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)), matutils\u001b[38;5;241m.\u001b[39munitvec(array(v2)\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))\n",
      "File \u001b[1;32mc:\\users\\minhh\\documents\\dev\\nlp\\.venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:337\u001b[0m, in \u001b[0;36mBaseKeyedVectors.__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;124;03m\"\"\"Get vector representation of `entities`.\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \n\u001b[0;32m    324\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(entities, string_types):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;66;03m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(entity) \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m entities])\n",
      "File \u001b[1;32mc:\\users\\minhh\\documents\\dev\\nlp\\.venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:455\u001b[0m, in \u001b[0;36mWordEmbeddingsKeyedVectors.get_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\minhh\\documents\\dev\\nlp\\.venv\\lib\\site-packages\\gensim\\models\\keyedvectors.py:452\u001b[0m, in \u001b[0;36mWordEmbeddingsKeyedVectors.word_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m word)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'covid' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model_dm.wv.n_similarity(['covid'],['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d9577e-ada4-46f7-923e-9f20afb3f321",
   "metadata": {},
   "source": [
    "# 5. Universal Text Representations\n",
    "These representations are very useful and popular in modern\u0002day NLP. However, based on our experience, here are a few important\n",
    "aspects to keep in mind while using them in your project:\n",
    "\n",
    "- All text representations are inherently biased based on what\n",
    "they saw in training data.\n",
    "- Unlike the basic vectorization approaches, pre-trained\n",
    "embeddings are generally large-sized files (several\n",
    "gigabytes), which may pose problems in certain deployment\n",
    "scenarios. This is something we need to address while using\n",
    "them, otherwise it can become an engineering bottleneck in\n",
    "performance.\n",
    "- Modeling language for a real-world application is more than\n",
    "capturing the information via word and sentence embeddings.\n",
    "- As we speak, neural text representation is an evolving area in\n",
    "NLP, with rapidly changing state of the art."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72479f23-3045-42ab-90f8-862c0855cb6d",
   "metadata": {},
   "source": [
    "# 6. Visualizing Embeddings\n",
    "Enter t-SNE, or *t-distributed Stochastic Neighboring\n",
    "Embedding*. It’s a technique used for visualizing high-dimensional data\n",
    "like embeddings by reducing them to two- or three-dimensional data.\n",
    "\n",
    "![](images/mnist.png)\n",
    "<center>Visualizing MNIST data using t-SNE</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca291c-5f28-4e16-ae3d-5a7c85e68a0b",
   "metadata": {},
   "source": [
    "Figure shows not only the\n",
    "position of the vectors of these words, but also an interesting\n",
    "observation between the vectors—the arrows capture the\n",
    "“relationship” between words. t-SNE visualization helps greatly in\n",
    "coming up with such nice observations.\n",
    "\n",
    "![](images/t-sne.png)\n",
    "<center>t-SNE visualization shows some interesting relationships</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69035532-e4dd-45e5-8747-378d99a48f9e",
   "metadata": {},
   "source": [
    "# 7. Handcrafted Feature Representations\n",
    "Clearly, measures such as “syntactic\n",
    "complexity,” “concreteness,” etc., cannot be calculated by only\n",
    "converting text into BoW or embedding representations. They have to\n",
    "be designed manually, keeping in mind both the domain knowledge and\n",
    "the ML algorithms to train the NLP models. This is why we call these\n",
    "*handcrafted feature representations*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a28737-5b66-499d-97ec-78ca1417f525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
