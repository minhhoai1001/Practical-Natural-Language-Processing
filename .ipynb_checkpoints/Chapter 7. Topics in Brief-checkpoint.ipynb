{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc8d3cc-ffdc-4278-ba47-fdc7f94125a6",
   "metadata": {},
   "source": [
    "# 1. Search and Information Retrieval\n",
    "Apart from this major function of storing data and ranking search\n",
    "results, several features in a modern search engine involve NLP\n",
    "- Spelling correction\n",
    "- Related queries\n",
    "- Snippet extraction\n",
    "- Biographical information extraction\n",
    "- Search results classification\n",
    "\n",
    "These two types of search engines are\n",
    "distinguished as follows:\n",
    "- Generic search engines, such as Google and Bing, that crawl\n",
    "the web and aim to cover as much as possible by constantly\n",
    "looking for new webpages\n",
    "-  search engines, where our search space is\n",
    "restricted to a smaller set of already existing documents\n",
    "within an organization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e15a6a-18bb-46c9-bb85-57c1c344177d",
   "metadata": {},
   "source": [
    "## Components of a Search Engine\n",
    "![](images/search-engine.png)\n",
    " <center>Early architecture of the Google search engine </center>\n",
    " - Crawler: Collects all the content for the search engine. The crawler’s job is\n",
    "to traverse the web following a bunch of seed URLs and build its\n",
    "collection of URLs through them in a breadth-first way. It visits\n",
    "each URL, saves a copy of the document, detects the outgoing\n",
    "hyperlinks, then adds them to the list of URLs to be visited next.\n",
    "- Indexer: Parses and stores the content that the crawler collects and builds\n",
    "an “index” so it can be searched and retrieved efficiently.\n",
    "- Searcher: Searches the index and ranks the search results for the user query\n",
    "based on the relevance of the results to the query.\n",
    "- Feedback: A fourth component, which is now common in all search engines,\n",
    "that tracks and analyzes user interactions with the search engine,\n",
    "such as click-throughs, time spent on searching and on each clicked\n",
    "result, etc., and uses it for continuous improvement of the search\n",
    "system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410904d5-0d71-4981-9aa9-81d410ef84ac",
   "metadata": {},
   "source": [
    "## A Typical Enterprise Search Pipeline\n",
    "- Crawling/content acquisition\n",
    "- Text normalization\n",
    "- Indexing\n",
    "\n",
    "The pipeline typically consists of the following steps:\n",
    "1. Query processing and execution: The search query is passed\n",
    "through the text normalization process as above. Once the\n",
    "query is framed, it’s executed, and results are retrieved and\n",
    "ranked according to some notion of relevance.\n",
    "2. Feedback and ranking: To evaluate search results and make\n",
    "them more relevant to the user, user behavior is recorded and\n",
    "analyzed, and signals such as click action on result and time\n",
    "spent on a result page are used to improve the ranking\n",
    "algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76286781-9ef5-4667-b942-2404468bcd20",
   "metadata": {},
   "source": [
    "## Setting Up a Search Engine: An Example\n",
    "This notebook shows how to use Elastic Search to index and search through data. We will use a dataset called CMU Book summaries [dataset](http://www.cs.cmu.edu/~dbamman/booksummaries.html).\n",
    "\n",
    "For this code to work, elastic search instance has to be running in the background. For this you need to follow these steps :\n",
    "\n",
    "Linux :\n",
    "\n",
    "1. Go to the elasticsearch-X.Y.Z/bin folder on your machine\n",
    "2. Run ./elasticsearch.\n",
    "\n",
    "Windows :\n",
    "\n",
    "1. Download the latest release\n",
    "2. Run .\\bin\\elasticsearch.bat\n",
    "\n",
    "[ElasticSearch Documentation](https://www.elastic.co/guide/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab74e3-fa24-4d4e-afd9-c43669f8840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610cadd3-1274-4c46-8da9-2e4a1fd5c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#elastic search instance has to be running on the machine. Default port is 9200. \n",
    "\n",
    "#Call the Elastic Search instance, and delete any pre-existing index\n",
    "es=Elasticsearch([{'host':'localhost','port':9200}])\n",
    "if es.indices.exists(index=\"myindex\"):\n",
    "    es.indices.delete(index='myindex', ignore=[400, 404]) #Deleting existing index for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4d6a90-aacc-48a9-a682-ffbaa30d457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build an index from booksummaries dataset. I am using only 500 documents for now.\n",
    "path = \"booksummaries.txt\" #Add your path.\n",
    "count = 1\n",
    "for line in open(path):\n",
    "    fields = line.split(\"\\t\")\n",
    "    doc = {'id' : fields[0],\n",
    "            'title': fields[2],\n",
    "            'author': fields[3],\n",
    "            'summary': fields[6]\n",
    "          }\n",
    "\n",
    "    res = es.index(index=\"myindex\", id=fields[0], body=doc)\n",
    "    count = count+1\n",
    "    if count%100 == 0:\n",
    "        print(\"indexed 100 documents\")\n",
    "    if count == 501:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4abf287-320f-4945-bc5c-51c366650bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to see how big is the index\n",
    "res = es.search(index=\"myindex\", body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Your index has %d entries\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86f0f2-f94e-4f7e-bd6a-751138633f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try a test query. The query searches \"summary\" field which contains the text\n",
    "#and does a full text query on that field.\n",
    "res = es.search(index=\"myindex\", body={\"query\": {\"match\": {\"summary\": \"animal\"}}})\n",
    "print(\"Your search returned %d results.\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe3e7d-40e8-42ad-9a53-13461392b88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the title field and summary field's first 100 characters for 2nd result\n",
    "print(res[\"hits\"][\"hits\"][2][\"_source\"][\"title\"])\n",
    "print(res[\"hits\"][\"hits\"][2][\"_source\"][\"summary\"][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2dee41-38bb-479a-bc77-69cb912c080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#match query considers both exact matches, and fuzzy matches and works as a OR query. \n",
    "#match_phrase looks for exact matches.\n",
    "while True:\n",
    "    query = input(\"Enter your search query: \")\n",
    "    if query == \"STOP\":\n",
    "        break\n",
    "    res = es.search(index=\"myindex\", body={\"query\": {\"match_phrase\": {\"summary\": query}}})\n",
    "    print(\"Your search returned %d results:\" % res['hits']['total']['value'])\n",
    "    for hit in res[\"hits\"][\"hits\"]:\n",
    "        print(hit[\"_source\"][\"title\"])\n",
    "        #to get a snippet 100 characters before and after the match\n",
    "        loc = hit[\"_source\"][\"summary\"].lower().index(query)\n",
    "        print(hit[\"_source\"][\"summary\"][:100])\n",
    "        print(hit[\"_source\"][\"summary\"][loc-100:loc+100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c3dfc-43e8-47b8-9bab-63ed8efcd6e3",
   "metadata": {},
   "source": [
    "## A Case Study: Book Store Search\n",
    "Imagine a scenario where we have a new e-commerce store focused\n",
    "on books and we have to build its search pipeline. We have metadata\n",
    "like author, title, and summary. The search functionality we saw earlier\n",
    "can serve as the baseline at the start. We can set up our own search\n",
    "engine backend or use online services like Elasticsearch or\n",
    "Elastic on Azure.\n",
    "\n",
    "This default search output might have a bunch of issues. For instance, it\n",
    "may show the results with exact query matches in title or summary to\n",
    "be higher than more relevant results that aren’t an exact match. Some of\n",
    "the exact matches might be poorly written books with bad reviews,\n",
    "which we’re not accounting for in our search ranking.\n",
    "\n",
    "We can incorporate real-world metrics that account for this into our\n",
    "search engine. For instance, the number of times a book is viewed and\n",
    "sold, the number of reviews, and the book’s rating can all be\n",
    "incorporated into the search ranking function. \n",
    "\n",
    "We should start collecting user interactions with the search engine to\n",
    "improve it further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa509e43-c157-45fc-a516-3c24f92a9161",
   "metadata": {},
   "source": [
    "# 2. Topic Modeling\n",
    "Topic models are used extensively for\n",
    "document clustering and organizing large collections of text data.\n",
    "They’re also useful for text classification.\n",
    "\n",
    "Topic modeling operationalizes this intuition. It tries to identify the\n",
    "“key” words (called “topics”) present in a text corpus without prior\n",
    "knowledge about it, unlike the rule-based text mining approaches that\n",
    "use regular expressions or dictionary-based keyword searching\n",
    "techniques. \n",
    "\n",
    "![](images/topic-modeling.png)\n",
    "<center>Illustration a of topic modeling visualization</center>\n",
    "\n",
    "Topic modeling generally refers to a collection of\n",
    "unsupervised statistical learning methods to discover latent topics in a\n",
    "large collection of text documents. Some of the popular topic modeling\n",
    "algorithms are latent Dirichlet allocation (LDA), latent semantic\n",
    "analysis (LSA), and probabilistic latent semantic analysis (PLSA). In\n",
    "practice, the technique that’s most commonly used is LDA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203383a8-3c77-4ed1-812d-510c635cc9eb",
   "metadata": {},
   "source": [
    "## Training a Topic Model: An Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd240f46-3f49-47d9-aff9-ebf18daaa903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a714b5-fecf-4ca1-bd15-22d6b7f0a3d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m [token\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39misalpha() \u001b[38;5;129;01mand\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stops]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# This is a sample path of your downloaded data set. This is currently set to a windows based path . \u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Please update it to your actual download path regradless of your choice of operating system \u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m)),data)\n\u001b[0;32m     12\u001b[0m summaries \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "#tokenize, remove stopwords, non-alphabetic words, lowercase\n",
    "def preprocess(textstring):\n",
    "   stops =  set(stopwords.words('english'))\n",
    "   tokens = word_tokenize(textstring)\n",
    "   return [token.lower() for token in tokens if token.isalpha() and token not in stops]\n",
    "\n",
    "# This is a sample path of your downloaded data set. This is currently set to a windows based path . \n",
    "# Please update it to your actual download path regradless of your choice of operating system \n",
    "\n",
    "data_path = os.path.join(os.path.dirname(os.path.abspath(__file__)),data)\n",
    "\n",
    "summaries = []\n",
    "for line in open(data_path, encoding=\"utf-8\"):\n",
    "   temp = line.split(\"\\t\")\n",
    "   summaries.append(preprocess(temp[6]))\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "\n",
    "dictionary = Dictionary(summaries)\n",
    "\n",
    "# Filter infrequent or too frequent words.\n",
    "\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(summary) for summary in summaries]\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "#Train the topic model\n",
    "\n",
    "model = LdaModel(corpus=corpus, id2word=id2word,iterations=400, num_topics=10)\n",
    "top_topics = list(model.top_topics(corpus))\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd141bf4-0f71-4c15-b094-2d4354a287ba",
   "metadata": {},
   "source": [
    "## What’s Next?\n",
    "In our experience, some of the use cases for topic models are:\n",
    "- Summarizing documents, tweets, etc., in the form of keywords based on learned topic distributions\n",
    "- Detecting social media trends over a period of time\n",
    "- Designing recommender systems for text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f200c2a2-ff61-4d6f-83b0-cd3913235d0f",
   "metadata": {},
   "source": [
    "# 3. Text Summarization\n",
    "Text summarization refers to the task of creating a summary of a\n",
    "longer piece of text. The goal of this task is to create a coherent\n",
    "summary that captures the key ideas in the text.\n",
    "\n",
    "- Extractive versus abstractive summarization\n",
    "- Query-focused versus query-independent summarization\n",
    "- Single-document versus multi-document summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3d38f-f946-43a7-902f-a79e4a0ba521",
   "metadata": {},
   "source": [
    "## Summarization Use Cases\n",
    "The most common use case for text summarization is\n",
    "a single-document, query-independent, extractive summarization. This\n",
    "is typically used to create short summaries of longer documents for\n",
    "human readers or a machine (e.g., in a search engine to index\n",
    "summaries instead of full texts).\n",
    "\n",
    "![](images/autotdr.png)\n",
    "<center>Screenshot of Reddit’s autotldr bot</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564184f-3894-4bb1-a6a9-091d5a98c1e6",
   "metadata": {},
   "source": [
    "**There are broadly two types of summarization — Extractive and Abstractive**\n",
    "1. Extractive— These approaches select sentences from the corpus that best represent it and arrange them to form a summary.\n",
    "2. Abstractive— These approaches use natural language techniques to summarize a text using novel sentences.\n",
    "\n",
    "## Setting Up a Summarizer: An Example\n",
    "**Summarization with Sumy**\\\n",
    "Sumy offers several algorithms and methods for summarization such as:\n",
    "1. Luhn – Heurestic method\n",
    "2. Latent Semantic Analysis\n",
    "4. LexRank – Unsupervised approach inspired by algorithms PageRank and HITS\n",
    "5. TextRank - Graph-based summarization technique with keyword extractions in from document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5016a5d6-8506-472b-911e-2358c2cc7aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextRankSummarizer:\n",
      "For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.\n",
      "Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\n",
      "------------------------------\n",
      "LexRankSummarizer:\n",
      "An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.\n",
      "The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".\n",
      "------------------------------\n",
      "LuhnSummarizer:\n",
      "Text summarization finds the most informative sentences in a document; [1] various methods of image summarization are the subject of ongoing research, with some looking to display the most representative images from a given collection or generating a video; [2][3][4] video summarization extracts the most important frames from the video content.\n",
      "A Class of Submodular Functions for Document Summarization \", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011 ^ Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning Mixtures of Submodular Functions for Image Collection Summarization , In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.\n",
      "------------------------------\n",
      "LsaSummarizer\n",
      "Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney's seminal paper.\n",
      "Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Code to summarize a given webpage using Sumy's TextRank implementation. \n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "num_sentences_in_summary = 2\n",
    "url = \"https://en.wikipedia.org/wiki/Automatic_summarization\"\n",
    "parser = HtmlParser.from_url(url, Tokenizer(\"english\"))\n",
    "\n",
    "summarizer_list=(\"TextRankSummarizer:\",\"LexRankSummarizer:\",\"LuhnSummarizer:\",\"LsaSummarizer\") #list of summarizers\n",
    "summarizers = [TextRankSummarizer(), LexRankSummarizer(), LuhnSummarizer(), LsaSummarizer()]\n",
    "\n",
    "for i,summarizer in enumerate(summarizers):\n",
    "    print(summarizer_list[i])\n",
    "    for sentence in summarizer(parser.document, num_sentences_in_summary):\n",
    "        print((sentence))\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1980d1c2-a371-434e-bbbc-e91205ae1fa7",
   "metadata": {},
   "source": [
    "## Summarization example with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af5dfab-11ae-4860-976d-32cbe38ad826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize:\n",
      " Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966.\n",
      "This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.\n",
      "However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.\n",
      "Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.\n",
      "In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing.\n",
      "------------------------------ \n",
      "Summarize Corpus\n",
      " [[(3, 1), (7, 1), (10, 1), (11, 1), (12, 1), (22, 1), (24, 1), (27, 1), (80, 1), (94, 1), (95, 1), (193, 1), (199, 1), (214, 1), (249, 1), (262, 1), (413, 1), (418, 1), (449, 1), (450, 1), (451, 1), (452, 1), (453, 1), (454, 1), (455, 1), (456, 1), (457, 1), (458, 1), (459, 1), (460, 1), (461, 1), (462, 1), (463, 1), (464, 1)], [(11, 1), (12, 1), (13, 1), (17, 3), (38, 1), (55, 1), (57, 1), (76, 1), (82, 2), (94, 1), (164, 1), (203, 1), (206, 2), (257, 1), (258, 1), (259, 1), (260, 1), (261, 1), (262, 1), (263, 1), (264, 1), (265, 1), (266, 1), (267, 1), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 1), (274, 1), (275, 1), (276, 1), (277, 1), (278, 1), (279, 1)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\minhh\\documents\\dev\\nlp\\.venv\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import summarize,summarize_corpus\n",
    "from gensim.summarization.textcleaner import split_sentences\n",
    "from gensim import corpora\n",
    "\n",
    "text = open(\"data/nlphistory.txt\").read()\n",
    "\n",
    "#summarize method extracts the most relevant sentences in a text\n",
    "print(\"Summarize:\\n\",summarize(text, word_count=200, ratio = 0.1))\n",
    "\n",
    "\n",
    "#the summarize_corpus selects the most important documents in a corpus:\n",
    "sentences = split_sentences(text)# Creates a corpus where each document is a sentence.\n",
    "tokens = [sentence.split() for sentence in sentences]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(sentence_tokens) for sentence_tokens in tokens]\n",
    "\n",
    "# Extracts the most important documents (shown here in BoW representation)\n",
    "print(\"-\"*30,\"\\nSummarize Corpus\\n\",summarize_corpus(corpus,ratio=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf41bb69-4956-40ea-b274-9100496341e2",
   "metadata": {},
   "source": [
    "The two parameters `word_count` and `ratio` we can adjust how much text the summarizer outputs\n",
    "\n",
    "1. `word_count`: maximum amount of words we want in the summary\n",
    "2. `ratio`: fraction of sentences in the original text should be returned as output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b21d6e8-8888-4da4-bcfa-22905339280c",
   "metadata": {},
   "source": [
    "## Practical Advice\n",
    "There are a few practical issues to keep in mind when deploying a\n",
    "summarizer:\n",
    "- Pre-processing steps like sentence splitting (or HTML parsing in the above example) play a very important role in what comes out as output summary. \n",
    "- Most summarization algorithms are sensitive to the size of the text given as input.You need to be aware of this limitation when using a summarizer with very large texts. A workaround could be to run the summarizer on partitions of the large text and stringing the summaries together. Another alternative could be to run the summarizer on the top M% and bottom N% of the text instead of the whole text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42099c2c-368c-41bc-ad7b-60110b2dcda3",
   "metadata": {},
   "source": [
    "# 4. Recommender Systems for Textual Data\n",
    "A common approach to building recommendation systems is a method\n",
    "called *collaborative filtering*. It shows recommendations to users\n",
    "based on their past history and on what users with similar profiles\n",
    "preferred in the past.\n",
    "\n",
    "## Creating a Book Recommender System: An Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0eb603b-b38e-4d11-bf7b-563e2890dd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\minhh\\documents\\dev\\nlp\\.venv\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb98f97a-ecd9-47a6-b1c5-981002829283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset’s README to understand the data format. \n",
    "\n",
    "data_path = \"data/booksummaries.txt\"\n",
    "mydata = {} #titles-summaries dictionary object\n",
    "for line in open(data_path, encoding=\"utf-8\"):\n",
    "    temp = line.split(\"\\t\")\n",
    "    mydata[temp[2]] = temp[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa386f8b-dfdd-4f49-8e0e-33485fe8a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the data for doc2vec, build and save a doc2vec model\n",
    "train_doc2vec = [TaggedDocument((word_tokenize(mydata[t])), tags=[t]) for t in mydata.keys()]\n",
    "model = Doc2Vec(vector_size=50, alpha=0.025, min_count=10, dm =1, epochs=100)\n",
    "model.build_vocab(train_doc2vec)\n",
    "model.train(train_doc2vec, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a55c775-6682-4aea-ba02-3c7084678977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the model to look for similar texts\n",
    "model= Doc2Vec.load(\"d2v.model\")\n",
    "\n",
    "#This is a sentence from the summary of “Animal Farm” on Wikipedia:\n",
    "#https://en.wikipedia.org/wiki/Animal_Farm\n",
    "sample = \"\"\"\n",
    "Napoleon enacts changes to the governance structure of the farm, replacing meetings with a committee of pigs who will run the farm.\n",
    " \"\"\"\n",
    "new_vector = model.infer_vector(word_tokenize(sample))\n",
    "sims = model.docvecs.most_similar([new_vector])\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8052cc-85af-428e-b97d-799b793698f5",
   "metadata": {},
   "source": [
    "## Practical Advice\n",
    "How do we know our recommendation system is working? In a real\u0002world project, the impact of recommendations can be measured by\n",
    "performance indicators, such as user click-through rates, conversion\n",
    "into a purchase (if relevant), customer engagement on the website, etc.\n",
    "\n",
    "Pre-processing decisions play a significant role\n",
    "in the recommendations served by our system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c1582-4856-4441-b84a-0d4ced3816ff",
   "metadata": {},
   "source": [
    "# 5. Machine Translation\n",
    "Machine translation (MT)—translating text from one language to\n",
    "another automatically—is one of the original problems of NLP\n",
    "research.\n",
    "\n",
    "Two example scenarios where MT may be required to develop solutions:\n",
    "- Our client’s products are used by people around the world\n",
    "who leave reviews on social media in multiple languages. Translate all the reviews into one language, and run\n",
    "sentiment analysis for that language.\n",
    "- We work with a lot of social media data (e.g., tweets) on a\n",
    "regular basis and notice that it’s unlike the kind of text we\n",
    "encounter in typical text documents. \n",
    "\n",
    "## Using a Machine Translation API: An Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c3230-cdaf-47bd-9b62-e87e03df949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, uuid, json\n",
    "\n",
    "# You will need a subscription key - you can use trial version\n",
    "# This will be user based\n",
    "\n",
    "subscription_key = \"XXXX\"\n",
    "endpoint = \"https://api-nam.cognitive.microsofttranslator.com\"\n",
    "path = '/translate?api-version=3.0'\n",
    "params = '&to=de' #From English to German (de)\n",
    "constructed_url = endpoint + path + params\n",
    "\n",
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': subscription_key,\n",
    "    'Content-type': 'application/json',\n",
    "    'X-ClientTraceId': str(uuid.uuid4())\n",
    "}\n",
    "\n",
    "body = [{'text' : 'How good is Machine Translation?'}]\n",
    "request = requests.post(constructed_url, headers=headers, json=body)\n",
    "response = request.json()\n",
    "\n",
    "print(json.dumps(response, sort_keys=True, indent=4, separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d513e-cb0d-4628-ad6d-08254f0e5e5d",
   "metadata": {},
   "source": [
    "## Practical Advice\n",
    "Don’t build your own MT system if you\n",
    "don’t have to. It’s more practical to make use of translation APIs.\n",
    "When using such APIs, it’s important to pay close attention to pricing\n",
    "policies. Considering the costs involved, it might be a good idea to\n",
    "store the translations of frequently used text (called a translation\n",
    "memory or a translation cache).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ec894-3a0c-47a2-8e9e-0ccf9387336c",
   "metadata": {},
   "source": [
    "# 6. Question-Answering Systems\n",
    "When searching online with a search engine such as Google or Bing,\n",
    "for some of the queries, we see “answers” along with a bunch of\n",
    "search results. These answers can be a few words or a listing or\n",
    "definition.\n",
    "\n",
    "NLP plays an important role in understanding the user query,\n",
    "deciding what kind of question it is and what kind of answer is needed,\n",
    "and identifying where the answers are in a given document after\n",
    "retrieving documents relevant to the query.\n",
    "\n",
    "![](images/answer-extraction.png)\n",
    "<center>Answer extraction</center>\n",
    "\n",
    "## Developing a Custom Question-Answering System\n",
    "Let’s say we’re asked to develop a question-answering system that\n",
    "answers all user questions about computers. We’ve identified a few\n",
    "websites with question-and-answer discussions (e.g., Stack Overflow)\n",
    "and have a crawler in place. The next step could be using\n",
    "text embeddings and performing a similarity-based search using\n",
    "Elasticsearch.\n",
    "\n",
    "## Looking for Deeper Answers\n",
    "Question\n",
    "answering using deep neural networks is very much an active area of\n",
    "research and is typically studied as a supervised ML problem using\n",
    "specific datasets designed for this task, such as the SQuAD\n",
    "dataset. DeepQA, which is a part of Allen NLP, is a popular\n",
    "library for developing experimental question-answering systems using\n",
    "DL architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2594cb75-7b7d-4e33-a947-96177de4f93d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
