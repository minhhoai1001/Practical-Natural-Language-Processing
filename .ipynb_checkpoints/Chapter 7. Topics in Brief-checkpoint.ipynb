{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc8d3cc-ffdc-4278-ba47-fdc7f94125a6",
   "metadata": {},
   "source": [
    "# 1. Search and Information Retrieval\n",
    "Apart from this major function of storing data and ranking search\n",
    "results, several features in a modern search engine involve NLP\n",
    "- Spelling correction\n",
    "- Related queries\n",
    "- Snippet extraction\n",
    "- Biographical information extraction\n",
    "- Search results classification\n",
    "\n",
    "These two types of search engines are\n",
    "distinguished as follows:\n",
    "- Generic search engines, such as Google and Bing, that crawl\n",
    "the web and aim to cover as much as possible by constantly\n",
    "looking for new webpages\n",
    "-  search engines, where our search space is\n",
    "restricted to a smaller set of already existing documents\n",
    "within an organization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e15a6a-18bb-46c9-bb85-57c1c344177d",
   "metadata": {},
   "source": [
    "## Components of a Search Engine\n",
    "![](images/search-engine.png)\n",
    " <center>Early architecture of the Google search engine </center>\n",
    " - Crawler: Collects all the content for the search engine. The crawler’s job is\n",
    "to traverse the web following a bunch of seed URLs and build its\n",
    "collection of URLs through them in a breadth-first way. It visits\n",
    "each URL, saves a copy of the document, detects the outgoing\n",
    "hyperlinks, then adds them to the list of URLs to be visited next.\n",
    "- Indexer: Parses and stores the content that the crawler collects and builds\n",
    "an “index” so it can be searched and retrieved efficiently.\n",
    "- Searcher: Searches the index and ranks the search results for the user query\n",
    "based on the relevance of the results to the query.\n",
    "- Feedback: A fourth component, which is now common in all search engines,\n",
    "that tracks and analyzes user interactions with the search engine,\n",
    "such as click-throughs, time spent on searching and on each clicked\n",
    "result, etc., and uses it for continuous improvement of the search\n",
    "system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410904d5-0d71-4981-9aa9-81d410ef84ac",
   "metadata": {},
   "source": [
    "## A Typical Enterprise Search Pipeline\n",
    "- Crawling/content acquisition\n",
    "- Text normalization\n",
    "- Indexing\n",
    "\n",
    "The pipeline typically consists of the following steps:\n",
    "1. Query processing and execution: The search query is passed\n",
    "through the text normalization process as above. Once the\n",
    "query is framed, it’s executed, and results are retrieved and\n",
    "ranked according to some notion of relevance.\n",
    "2. Feedback and ranking: To evaluate search results and make\n",
    "them more relevant to the user, user behavior is recorded and\n",
    "analyzed, and signals such as click action on result and time\n",
    "spent on a result page are used to improve the ranking\n",
    "algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76286781-9ef5-4667-b942-2404468bcd20",
   "metadata": {},
   "source": [
    "## Setting Up a Search Engine: An Example\n",
    "This notebook shows how to use Elastic Search to index and search through data. We will use a dataset called CMU Book summaries [dataset](http://www.cs.cmu.edu/~dbamman/booksummaries.html).\n",
    "\n",
    "For this code to work, elastic search instance has to be running in the background. For this you need to follow these steps :\n",
    "\n",
    "Linux :\n",
    "\n",
    "1. Go to the elasticsearch-X.Y.Z/bin folder on your machine\n",
    "2. Run ./elasticsearch.\n",
    "\n",
    "Windows :\n",
    "\n",
    "1. Download the latest release\n",
    "2. Run .\\bin\\elasticsearch.bat\n",
    "\n",
    "[ElasticSearch Documentation](https://www.elastic.co/guide/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab74e3-fa24-4d4e-afd9-c43669f8840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch \n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610cadd3-1274-4c46-8da9-2e4a1fd5c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#elastic search instance has to be running on the machine. Default port is 9200. \n",
    "\n",
    "#Call the Elastic Search instance, and delete any pre-existing index\n",
    "es=Elasticsearch([{'host':'localhost','port':9200}])\n",
    "if es.indices.exists(index=\"myindex\"):\n",
    "    es.indices.delete(index='myindex', ignore=[400, 404]) #Deleting existing index for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4d6a90-aacc-48a9-a682-ffbaa30d457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build an index from booksummaries dataset. I am using only 500 documents for now.\n",
    "path = \"booksummaries.txt\" #Add your path.\n",
    "count = 1\n",
    "for line in open(path):\n",
    "    fields = line.split(\"\\t\")\n",
    "    doc = {'id' : fields[0],\n",
    "            'title': fields[2],\n",
    "            'author': fields[3],\n",
    "            'summary': fields[6]\n",
    "          }\n",
    "\n",
    "    res = es.index(index=\"myindex\", id=fields[0], body=doc)\n",
    "    count = count+1\n",
    "    if count%100 == 0:\n",
    "        print(\"indexed 100 documents\")\n",
    "    if count == 501:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4abf287-320f-4945-bc5c-51c366650bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to see how big is the index\n",
    "res = es.search(index=\"myindex\", body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Your index has %d entries\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86f0f2-f94e-4f7e-bd6a-751138633f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try a test query. The query searches \"summary\" field which contains the text\n",
    "#and does a full text query on that field.\n",
    "res = es.search(index=\"myindex\", body={\"query\": {\"match\": {\"summary\": \"animal\"}}})\n",
    "print(\"Your search returned %d results.\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe3e7d-40e8-42ad-9a53-13461392b88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the title field and summary field's first 100 characters for 2nd result\n",
    "print(res[\"hits\"][\"hits\"][2][\"_source\"][\"title\"])\n",
    "print(res[\"hits\"][\"hits\"][2][\"_source\"][\"summary\"][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2dee41-38bb-479a-bc77-69cb912c080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#match query considers both exact matches, and fuzzy matches and works as a OR query. \n",
    "#match_phrase looks for exact matches.\n",
    "while True:\n",
    "    query = input(\"Enter your search query: \")\n",
    "    if query == \"STOP\":\n",
    "        break\n",
    "    res = es.search(index=\"myindex\", body={\"query\": {\"match_phrase\": {\"summary\": query}}})\n",
    "    print(\"Your search returned %d results:\" % res['hits']['total']['value'])\n",
    "    for hit in res[\"hits\"][\"hits\"]:\n",
    "        print(hit[\"_source\"][\"title\"])\n",
    "        #to get a snippet 100 characters before and after the match\n",
    "        loc = hit[\"_source\"][\"summary\"].lower().index(query)\n",
    "        print(hit[\"_source\"][\"summary\"][:100])\n",
    "        print(hit[\"_source\"][\"summary\"][loc-100:loc+100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530c3dfc-43e8-47b8-9bab-63ed8efcd6e3",
   "metadata": {},
   "source": [
    "## A Case Study: Book Store Search\n",
    "Imagine a scenario where we have a new e-commerce store focused\n",
    "on books and we have to build its search pipeline. We have metadata\n",
    "like author, title, and summary. The search functionality we saw earlier\n",
    "can serve as the baseline at the start. We can set up our own search\n",
    "engine backend or use online services like Elasticsearch or\n",
    "Elastic on Azure.\n",
    "\n",
    "This default search output might have a bunch of issues. For instance, it\n",
    "may show the results with exact query matches in title or summary to\n",
    "be higher than more relevant results that aren’t an exact match. Some of\n",
    "the exact matches might be poorly written books with bad reviews,\n",
    "which we’re not accounting for in our search ranking.\n",
    "\n",
    "We can incorporate real-world metrics that account for this into our\n",
    "search engine. For instance, the number of times a book is viewed and\n",
    "sold, the number of reviews, and the book’s rating can all be\n",
    "incorporated into the search ranking function. \n",
    "\n",
    "We should start collecting user interactions with the search engine to\n",
    "improve it further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa509e43-c157-45fc-a516-3c24f92a9161",
   "metadata": {},
   "source": [
    "# 2. Topic Modeling\n",
    "Topic models are used extensively for\n",
    "document clustering and organizing large collections of text data.\n",
    "They’re also useful for text classification.\n",
    "\n",
    "Topic modeling operationalizes this intuition. It tries to identify the\n",
    "“key” words (called “topics”) present in a text corpus without prior\n",
    "knowledge about it, unlike the rule-based text mining approaches that\n",
    "use regular expressions or dictionary-based keyword searching\n",
    "techniques. \n",
    "\n",
    "![](images/topic-modeling.png)\n",
    "<center>Illustration a of topic modeling visualization</center>\n",
    "\n",
    "Topic modeling generally refers to a collection of\n",
    "unsupervised statistical learning methods to discover latent topics in a\n",
    "large collection of text documents. Some of the popular topic modeling\n",
    "algorithms are latent Dirichlet allocation (LDA), latent semantic\n",
    "analysis (LSA), and probabilistic latent semantic analysis (PLSA). In\n",
    "practice, the technique that’s most commonly used is LDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bce1e9-7356-4742-8e9d-a8dd2b8a3413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
