{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "022dc5df-b01e-4b18-8fa9-664a680daa7c",
   "metadata": {},
   "source": [
    "In this chapter, we’ll discuss various IE tasks and the methods for\n",
    "implementing them for our applications. We’ll start with a brief\n",
    "historical background, followed by an overview of different IE tasks\n",
    "and applications of IE in the real world. We’ll then introduce the\n",
    "typical NLP processing pipeline for solving any IE task and move on\n",
    "to discuss how to solve specific IE tasks—key phrase extraction,\n",
    "named entity recognition, named entity disambiguation and linking, and\n",
    "relationship extraction—along with some practical advice on\n",
    "implementing them in your projects. We’ll then present a case study of\n",
    "how IE is used in a real-world scenario and briefly cover other\n",
    "advanced IE tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b711a9-b156-4640-b70d-891b2193f66b",
   "metadata": {},
   "source": [
    "# 1. IE Applications\n",
    "- Tagging news and other content\n",
    "- Chatbots\n",
    "- Applications in social media\n",
    "- Extracting data from forms and receipts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715f9d43-fef0-46c4-b7de-20b44c5cbbad",
   "metadata": {},
   "source": [
    "# 2. IE Tasks\n",
    "The overarching goal of IE is to extract “knowledge” from\n",
    "text, and each of these tasks provides different information to do that.\n",
    "- Keyword or keyphrase extraction (KPE).\n",
    "- Named entity recognition (NER).\n",
    "- Event extraction\n",
    "- Temporal information extraction\n",
    "- Template filling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de1cdcb-477b-4664-bcc3-c0b42698fea6",
   "metadata": {},
   "source": [
    "# 3. The General Pipeline for IE\n",
    "![](images/pipeline-IE.png)\n",
    "<center>IE pipeline illustrating NLP processing needed for some IE tasks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0c68e-27e8-4819-8892-825f6d0df23f",
   "metadata": {},
   "source": [
    "# 4. Keyphrase Extraction\n",
    "Keyword and phrase extraction, as the name indicates, is the IE task\n",
    "concerned with extracting important words and phrases that capture the\n",
    "gist of the text from a given text document.\n",
    "\n",
    "## Implementing KPE\n",
    "The Python library textacy, built on top of the well-known library\n",
    "spaCy, contains implementations for some of the common graph based keyword and phrase extraction algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62eaed4-a075-4cde-9c24-0c33a79a79d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy.ke\n",
    "from textacy import *\n",
    "\n",
    "#Load a spacy model, which will be used for all further processing.\n",
    "en = textacy.load_spacy_lang(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6fb721-047a-49bf-aa03-f1f145612621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us use a sample text file, nlphistory.txt, which is the text from the history section of Wikipedia's\n",
    "#page on Natural Language Processing \n",
    "#https://en.wikipedia.org/wiki/Natural_language_processing\n",
    "try :\n",
    "    from google.colab import files\n",
    "    uploaded=files.upload()\n",
    "    mytext = open('nlphistory.txt').read()\n",
    "\n",
    "except ModuleNotFoundError :\n",
    "    mytext = open('Data/nlphistory.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7754d355-585b-4701-be2f-ff46ef9e14aa",
   "metadata": {},
   "source": [
    "Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable.\n",
    "Saving nlphistory.txt to nlphistory.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e72c8-f89d-414c-b88b-e5eb026c7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the text into a spacy document.\n",
    "doc = textacy.make_spacy_doc(mytext, lang=en)\n",
    "textacy.ke.textrank(doc, topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4133429-37b7-416d-9796-982d0aea7e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the keywords using TextRank algorithm, as implemented in Textacy.\n",
    "print(\"Textrank output: \", [kps for kps, weights in textacy.ke.textrank(doc, normalize=\"lemma\", topn=5)])\\\n",
    "#Print the key words and phrases, using SGRank algorithm, as implemented in Textacy\n",
    "print(\"SGRank output: \", [kps for kps, weights in textacy.ke.sgrank(doc, topn=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca75e4-7495-4a60-96f3-686ffb9c8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To address the issue of overlapping key phrases, textacy has a function: aggregage_term_variants.\n",
    "#Choosing one of the grouped terms per item will give us a list of non-overlapping key phrases!\n",
    "terms = set([term for term,weight in textacy.ke.sgrank(doc)])\n",
    "print(textacy.ke.utils.aggregate_term_variants(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92731ea0-67aa-42dc-8c8f-11ffc6d8b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A way to look at key phrases is just consider all noun chunks as potential ones. \n",
    "#However, keep in mind this will result in a lot of phrases, and no way to rank them!\n",
    "\n",
    "print([chunk for chunk in textacy.extract.noun_chunks(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1a09aa-caa9-491e-8dac-e70ff97c1282",
   "metadata": {},
   "source": [
    "Textacy also has a bunch of other information extraction functions, many of them based on regular expression patterns and heuristics to address extracting specific expressions such as acronyms and quotations. Apart from these, we can also extract matching custom regular expressions including POS tag patterns, or look for statements involving an entity, subject-verb-object tuples etc. We will discuss some of these as they come, in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2420ea3f-08d1-4bb1-9a82-3dcc0b31a065",
   "metadata": {},
   "source": [
    "# 5. Named Entity Recognition\n",
    "NER refers to the IE task of identifying the entities in a document.\n",
    "Entities are typically names of persons, locations, and organizations,\n",
    "and other specialized strings, such as money expressions, dates,\n",
    "products, names/numbers of laws or articles, and so on. NER is an\n",
    "important step in the pipeline of several NLP applications involving\n",
    "information extraction.\n",
    "\n",
    "![](images/ner-example.png)\n",
    "<center>NER example using the displaCy visualizer</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba25f50-4f03-429f-98bf-24138d9e61c6",
   "metadata": {},
   "source": [
    "## Building an NER System\n",
    "**Conditional random fields (CRFs)** is one of the popular sequence\n",
    "classifier training algorithms.\n",
    "To perform sequence classification, we need data in a format that\n",
    "allows us to model the context. Typical training data for NER looks\n",
    "like Figure below, which is a sentence from the CONLL-03 dataset.\n",
    "The labels in the figure follow what’s known as a BIO notation: **B**\n",
    "indicates the beginning of an entity; **I**, inside an entity, indicates when\n",
    "entities comprise more than one word; and **O**, other, indicates non entities.\n",
    "\n",
    "![](images/NER-tag.png)\n",
    "<center>NER training data format example</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e118dc3a-bb41-4bfb-8715-ceecfabba912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the necessary imports\n",
    "from nltk.tag import pos_tag\n",
    "from sklearn_crfsuite import CRF, metrics\n",
    "from sklearn.metrics import make_scorer,confusion_matrix\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score,classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4139c53-a4b8-41c4-97b8-428aff07173b",
   "metadata": {},
   "source": [
    "### Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f3c14a-97a5-47a8-8965-43faa7519878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the training/testing data. \n",
    "input: conll format data, but with only 2 tab separated colums - words and NEtags.\n",
    "output: A list where each item is 2 lists.  sentence as a list of tokens, NER tags as a list for each token.\n",
    "\"\"\"\n",
    "def load__data_conll(file_path):\n",
    "    myoutput,words,tags = [],[],[]\n",
    "    fh = open(file_path)\n",
    "    for line in fh:\n",
    "        line = line.strip()\n",
    "        if \"\\t\" not in line:\n",
    "            #Sentence ended.\n",
    "            myoutput.append([words,tags])\n",
    "            words,tags = [],[]\n",
    "        else:\n",
    "            word, tag = line.split(\"\\t\")\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "    fh.close()\n",
    "    return myoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d19871-95eb-48c8-b6a0-09194bd827f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get features for all words in the sentence\n",
    "Features:\n",
    "- word context: a window of 2 words on either side of the current word, and current word.\n",
    "- POS context: a window of 2 POS tags on either side of the current word, and current tag. \n",
    "input: sentence as a list of tokens.\n",
    "output: list of dictionaries. each dict represents features for that word.\n",
    "\"\"\"\n",
    "def sent2feats(sentence):\n",
    "    feats = []\n",
    "    sen_tags = pos_tag(sentence) #This format is specific to this POS tagger!\n",
    "    for i in range(0,len(sentence)):\n",
    "        word = sentence[i]\n",
    "        wordfeats = {}\n",
    "       #word features: word, prev 2 words, next 2 words in the sentence.\n",
    "        wordfeats['word'] = word\n",
    "        if i == 0:\n",
    "            wordfeats[\"prevWord\"] = wordfeats[\"prevSecondWord\"] = \"<S>\"\n",
    "        elif i==1:\n",
    "            wordfeats[\"prevWord\"] = sentence[0]\n",
    "            wordfeats[\"prevSecondWord\"] = \"</S>\"\n",
    "        else:\n",
    "            wordfeats[\"prevWord\"] = sentence[i-1]\n",
    "            wordfeats[\"prevSecondWord\"] = sentence[i-2]\n",
    "        #next two words as features\n",
    "        if i == len(sentence)-2:\n",
    "            wordfeats[\"nextWord\"] = sentence[i+1]\n",
    "            wordfeats[\"nextNextWord\"] = \"</S>\"\n",
    "        elif i==len(sentence)-1:\n",
    "            wordfeats[\"nextWord\"] = \"</S>\"\n",
    "            wordfeats[\"nextNextWord\"] = \"</S>\"\n",
    "        else:\n",
    "            wordfeats[\"nextWord\"] = sentence[i+1]\n",
    "            wordfeats[\"nextNextWord\"] = sentence[i+2]\n",
    "        \n",
    "        #POS tag features: current tag, previous and next 2 tags.\n",
    "        wordfeats['tag'] = sen_tags[i][1]\n",
    "        if i == 0:\n",
    "            wordfeats[\"prevTag\"] = wordfeats[\"prevSecondTag\"] = \"<S>\"\n",
    "        elif i == 1:\n",
    "            wordfeats[\"prevTag\"] = sen_tags[0][1]\n",
    "            wordfeats[\"prevSecondTag\"] = \"</S>\"\n",
    "        else:\n",
    "            wordfeats[\"prevTag\"] = sen_tags[i - 1][1]\n",
    "\n",
    "            wordfeats[\"prevSecondTag\"] = sen_tags[i - 2][1]\n",
    "            # next two words as features\n",
    "        if i == len(sentence) - 2:\n",
    "            wordfeats[\"nextTag\"] = sen_tags[i + 1][1]\n",
    "            wordfeats[\"nextNextTag\"] = \"</S>\"\n",
    "        elif i == len(sentence) - 1:\n",
    "            wordfeats[\"nextTag\"] = \"</S>\"\n",
    "            wordfeats[\"nextNextTag\"] = \"</S>\"\n",
    "        else:\n",
    "            wordfeats[\"nextTag\"] = sen_tags[i + 1][1]\n",
    "            wordfeats[\"nextNextTag\"] = sen_tags[i + 2][1]\n",
    "        #That is it! You can add whatever you want!\n",
    "        feats.append(wordfeats)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5506bbe5-a852-498b-a609-47b499b5b66b",
   "metadata": {},
   "source": [
    "### Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f243cf27-bc66-422a-90c2-ee0e34617eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract features from the conll data, after loading it.\n",
    "def get_feats_conll(conll_data):\n",
    "    feats = []\n",
    "    labels = []\n",
    "    for sentence in conll_data:\n",
    "        feats.append(sent2feats(sentence[0]))\n",
    "        labels.append(sentence[1])\n",
    "    return feats, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7a49af-3439-4b13-8b03-3c50195ff442",
   "metadata": {},
   "source": [
    "### Training a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45f461d8-cf03-4b50-b996-ce1749a77c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a sequence model\n",
    "def train_seq(X_train,Y_train,X_dev,Y_dev):\n",
    "   # crf = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=50, all_possible_states=True)\n",
    "    crf = CRF(algorithm='lbfgs', c1=0.1, c2=10, max_iterations=50)#, all_possible_states=True)\n",
    "    #Just to fit on training data\n",
    "    crf.fit(X_train, Y_train)\n",
    "    labels = list(crf.classes_)\n",
    "    #testing:\n",
    "    y_pred = crf.predict(X_dev)\n",
    "    sorted_labels = sorted(labels, key=lambda name: (name[1:], name[0]))\n",
    "    print(metrics.flat_f1_score(Y_dev, y_pred,average='weighted'))\n",
    "    # print(metrics.flat_classification_report(Y_dev, y_pred))\n",
    "    #print(metrics.sequence_accuracy_score(Y_dev, y_pred))\n",
    "    get_confusion_matrix(Y_dev, y_pred,labels=sorted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b514285-0c13-4e6a-8374-06e1a3f534c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cm(cm, labels):\n",
    "    print(\"\\n\")\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=\" \")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\" \")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=\" \")\n",
    "        sum = 0\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}.0f\".format(columnwidth) % cm[i, j]\n",
    "            sum =  sum + int(cell)\n",
    "            print(cell, end=\" \")\n",
    "        print(sum) #Prints the total number of instances per cat at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd85d90d-6127-40fa-902d-f2e0f813b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python-crfsuite does not have a confusion matrix function, \n",
    "#so writing it using sklearn's confusion matrix and print_cm from github\n",
    "def get_confusion_matrix(y_true,y_pred,labels):\n",
    "    trues,preds = [], []\n",
    "    for yseq_true, yseq_pred in zip(y_true, y_pred):\n",
    "        trues.extend(yseq_true)\n",
    "        preds.extend(yseq_pred)\n",
    "    print_cm(confusion_matrix(trues,preds),labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cab91c-7d5c-4738-9862-6317ee3e52a8",
   "metadata": {},
   "source": [
    "### Call all our functions inside the main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c98ff5f-7491-4e05-92cc-7f0f4a94abf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a Sequence classification model with CRF\n",
      "0.9255103670420659\n",
      "\n",
      "\n",
      "                O  B-LOC  I-LOC B-MISC I-MISC  B-ORG  I-ORG  B-PER  I-PER \n",
      "         O   1276     36     95     98      1      1     14      4    143 1668\n",
      "     B-LOC     48    217     56     19      1      2     13      2    344 702\n",
      "     I-LOC    236     48    932    151      0      3     20      6    265 1661\n",
      "    B-MISC    138      5     90   1238      1      3     44     12     86 1617\n",
      "    I-MISC      6      1      0      0    124      5     52     37     32 257\n",
      "     B-ORG      1      4      2      0      4    109     29      9     58 216\n",
      "     I-ORG     15      2     21      8     18     15    588     92     76 835\n",
      "     B-PER      1      0      2      0     16      4     83   1024     26 1156\n",
      "     I-PER    118     22    193     88      3     32    224     64  37579 38323\n",
      "Done with sequence model\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    try:\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()\n",
    "        # files are present in Data/conlldata\n",
    "        train_path = 'train.txt'\n",
    "        test_path = 'test.txt'\n",
    "    except:\n",
    "        train_path = 'data/conlldata/train.txt'\n",
    "        test_path = 'data/conlldata/test.txt'\n",
    "        \n",
    "    conll_train = load__data_conll(train_path)\n",
    "    conll_dev = load__data_conll(test_path)\n",
    "    \n",
    "    print(\"Training a Sequence classification model with CRF\")\n",
    "    feats, labels = get_feats_conll(conll_train)\n",
    "    devfeats, devlabels = get_feats_conll(conll_dev)\n",
    "    train_seq(feats, labels, devfeats, devlabels)\n",
    "    print(\"Done with sequence model\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a2e2b9-18bb-448e-a6f3-81cd66b379ef",
   "metadata": {},
   "source": [
    "This is pretty good. We already have a model which has an F-score of 92%!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cffd17f-02b4-4b19-a920-891c06ea3de8",
   "metadata": {},
   "source": [
    "## NER Using an Existing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "006334df-3e42-45cf-9691-45feb0e89d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAN FRANCISCO \t GPE\n",
      "Apple \t ORG\n",
      "last year \t DATE\n",
      "$252 billion \t MONEY\n",
      "$100 billion \t MONEY\n",
      "Tuesday \t DATE\n",
      "Apple \t ORG\n",
      "$75 billion \t MONEY\n",
      "first \t ORDINAL\n",
      "Luca Maestri \t PERSON\n",
      "Apple \t ORG\n"
     ]
    }
   ],
   "source": [
    "#Problems of NER illustration through Spacy.\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "mytext = \"\"\"SAN FRANCISCO — Shortly after Apple used a new tax law last year to bring back most of the $252 billion it had held abroad, the company said it would buy back $100 billion of its stock.\n",
    "\n",
    "On Tuesday, Apple announced its plans for another major chunk of the money: It will buy back a further $75 billion in stock.\n",
    "\n",
    "“Our first priority is always looking after the business and making sure we continue to grow and invest,” Luca Maestri, Apple’s finance chief, said in an interview. “If there is excess cash, then obviously we want to return it to investors.”\n",
    "\n",
    "Apple’s record buybacks should be welcome news to shareholders, as the stock price is likely to climb. But the buybacks could also expose the company to more criticism that the tax cuts it received have mostly benefited investors and executives.\n",
    "\"\"\"\n",
    "doc = nlp(mytext)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"\\t\", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f8c455-e223-41f7-bd0a-742450bebeb4",
   "metadata": {},
   "source": [
    "## Practical Advice\n",
    "Despite the fact that state-of-the-art NER\n",
    "is highly accurate (with F1 scores over 90% using standard evaluation\n",
    "frameworks for NER in NLP research), there are several issues to\n",
    "keep in mind when using NER in our own software applications. Here\n",
    "are a couple caveats based on our own experience with developing\n",
    "NER systems:\n",
    "- NER is very sensitive to the format of its input.\n",
    "- NER is also very sensitive to the accuracy of the prior steps\n",
    "in its processing pipeline: sentence splitting, tokenization, and\n",
    "POS tagging.\n",
    "\n",
    "Despite such shortcomings, NER is immensely useful for many IE\n",
    "scenarios, such as content tagging, search, and mining social media to\n",
    "identify customer feedback about specific products, to name a few."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19bc544-739e-4301-93f4-34e96a94f407",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how we can leverage BERT to perform NER on conll2003 dataset.\n",
    "This notebook requires a GPU to get setup. We suggest you to run this on your local machine only if you have a GPU setup or else you can use google colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862ad5a-85d1-4c94-a1f0-2821f3187fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing required packages\n",
    "try :\n",
    "    from google.colab import files\n",
    "    %tensorflow_version 1.x\n",
    "    \n",
    "except ModuleNotFoundError :\n",
    "    Print(\"Not Using Colab\")\n",
    "    \n",
    "!pip install pytorch-pretrained-bert==0.4.0\n",
    "!pip install seqeval==0.0.12\n",
    "\n",
    "#importing packages for string processing,dataframe handling, array manipulations, etc\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#importing all the pytorch packages\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n",
    "\n",
    "#importing additonal packages to aid preprocessing of data\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#importing packages to calculate the f1_score of our model\n",
    "from seqeval.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c46d33-b7e4-474f-9090-7ac16b1c0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uploading data into google colab\n",
    "#upload the test.txt and train.txt files respectively\n",
    "try :\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "\n",
    "except ModuleNotFoundError :\n",
    "    print(\"Not using Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6526d17-5a82-4e93-afc9-2cfba8cb0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the training/testing data. \n",
    "input: conll format data, but with only 2 tab separated colums - words and NEtags.\n",
    "output: A list where each item is 2 lists.  sentence as a list of tokens, NER tags as a list for each token.\n",
    "\"\"\"\n",
    "#functions for preparing the data in the *.txt files\n",
    "def load__data_conll(file_path):\n",
    "    myoutput,words,tags = [],[],[]\n",
    "    fh = open(file_path)\n",
    "    for line in fh:\n",
    "        line = line.strip()\n",
    "        if \"\\t\" not in line:\n",
    "            #Sentence ended.\n",
    "            myoutput.append([words,tags])\n",
    "            words,tags = [],[]\n",
    "        else:\n",
    "            word, tag = line.split(\"\\t\")\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "    fh.close()\n",
    "    return myoutput\n",
    "\n",
    "\"\"\"\n",
    "Get features for all words in the sentence\n",
    "Features:\n",
    "- word context: a window of 2 words on either side of the current word, and current word.\n",
    "- POS context: a window of 2 POS tags on either side of the current word, and current tag. \n",
    "input: sentence as a list of tokens.\n",
    "output: list of dictionaries. each dict represents features for that word.\n",
    "\"\"\"\n",
    "def sent2feats(sentence):\n",
    "    feats = []\n",
    "    sen_tags = pos_tag(sentence) #This format is specific to this POS tagger!\n",
    "    for i in range(0,len(sentence)):\n",
    "        word = sentence[i]\n",
    "        wordfeats = {}\n",
    "       #word features: word, prev 2 words, next 2 words in the sentence.\n",
    "        wordfeats['word'] = word\n",
    "        if i == 0:\n",
    "            wordfeats[\"prevWord\"] = wordfeats[\"prevSecondWord\"] = \"<S>\"\n",
    "        elif i==1:\n",
    "            wordfeats[\"prevWord\"] = sentence[0]\n",
    "            wordfeats[\"prevSecondWord\"] = \"</S>\"\n",
    "        else:\n",
    "            wordfeats[\"prevWord\"] = sentence[i-1]\n",
    "            wordfeats[\"prevSecondWord\"] = sentence[i-2]\n",
    "        #next two words as features\n",
    "        if i == len(sentence)-2:\n",
    "            wordfeats[\"nextWord\"] = sentence[i+1]\n",
    "            wordfeats[\"nextNextWord\"] = \"</S>\"\n",
    "        elif i==len(sentence)-1:\n",
    "            wordfeats[\"nextWord\"] = \"</S>\"\n",
    "            wordfeats[\"nextNextWord\"] = \"</S>\"\n",
    "        else:\n",
    "            wordfeats[\"nextWord\"] = sentence[i+1]\n",
    "            wordfeats[\"nextNextWord\"] = sentence[i+2]\n",
    "        \n",
    "        #POS tag features: current tag, previous and next 2 tags.\n",
    "        wordfeats['tag'] = sen_tags[i][1]\n",
    "        if i == 0:\n",
    "            wordfeats[\"prevTag\"] = wordfeats[\"prevSecondTag\"] = \"<S>\"\n",
    "        elif i == 1:\n",
    "            wordfeats[\"prevTag\"] = sen_tags[0][1]\n",
    "            wordfeats[\"prevSecondTag\"] = \"</S>\"\n",
    "        else:\n",
    "            wordfeats[\"prevTag\"] = sen_tags[i - 1][1]\n",
    "\n",
    "            wordfeats[\"prevSecondTag\"] = sen_tags[i - 2][1]\n",
    "            # next two words as features\n",
    "        if i == len(sentence) - 2:\n",
    "            wordfeats[\"nextTag\"] = sen_tags[i + 1][1]\n",
    "            wordfeats[\"nextNextTag\"] = \"</S>\"\n",
    "        elif i == len(sentence) - 1:\n",
    "            wordfeats[\"nextTag\"] = \"</S>\"\n",
    "            wordfeats[\"nextNextTag\"] = \"</S>\"\n",
    "        else:\n",
    "            wordfeats[\"nextTag\"] = sen_tags[i + 1][1]\n",
    "            wordfeats[\"nextNextTag\"] = sen_tags[i + 2][1]\n",
    "        #That is it! You can add whatever you want!\n",
    "        feats.append(wordfeats)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9875e3df-55a5-4e61-97f8-9b4340bd61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the data by calling the functions\n",
    "try :\n",
    "    from google.colab import files\n",
    "    train_path = 'train.txt'\n",
    "    test_path = 'test.txt' \n",
    "\n",
    "except ModuleNotFoundError :\n",
    "    train_path = 'Data/conlldata/train.txt'\n",
    "    test_path = 'Data/conlldata/test.txt'\n",
    "\n",
    "    \n",
    "conll_train = load__data_conll(train_path)\n",
    "conll_test = load__data_conll(test_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5df551-67a6-4c45-bb07-a2390a260da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT needs us to pre-process the data in a particular way.\n",
    "#Lets take the raw data from the txt files\n",
    "df_train = pd.read_csv(train_path, engine=\"python\",delimiter=\"\\t\",header=None,encoding='utf-8',error_bad_lines=False)\n",
    "df_test = pd.read_csv(test_path, engine=\"python\",delimiter=\"\\t\",encoding='utf-8',header=None, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0f6f3-ff45-42da-8bb0-bdbbbe592baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69813b23-4e51-4a28-955f-5e5badb32c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge \n",
    "df = pd.merge(df_train,df_test)\n",
    "label = list(df[1].values)#we will be using this to make a set of all unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd4933-bb5f-4d20-9a3b-9f0697bc8ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(conll_train).shape#calculating the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a4e0f-b455-4d02-839b-03c072cc42bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(conll_test).shape#calculating the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f188979d-3008-43e0-b353-387b4dbf3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def untokenize(words):\n",
    "    \"\"\"\n",
    "    Untokenizing a text undoes the tokenizing operation, restoring\n",
    "    punctuation and spaces to the places that people expect them to be.\n",
    "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "    except for line breaks.\n",
    "    \"\"\"\n",
    "    text = ' '.join(words)\n",
    "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "         \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    return step6.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af88718-5a4b-42ef-955f-73c674dec83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets convert them to dataframs for easier handling\n",
    "df_train = pd.DataFrame(conll_train,columns=[\"sentence\",\"labels\"])\n",
    "df_test = pd.DataFrame(conll_test,columns=[\"sentence\",\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e4988-cecb-4dd0-93d8-0763e81f43f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all the sentences and labels present in both test and train\n",
    "sentences = list(df_train['sentence'])+list(df_test['sentence'])\n",
    "print(\"No of sentences:\",len(sentences))\n",
    "labels = list(df_train['labels'])+list(df_test['labels']) \n",
    "print(\"No of labels:\",len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00db73a6-dbad-41fd-aefd-e78b156b08a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [untokenize(sent) for sent in sentences]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2644ee-e8f7-4f66-8e06-12817da5e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up pytorch to use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "#prescribed configurations that we need to fix for BERT.\n",
    "MAX_LEN = 75\n",
    "bs = 32\n",
    "\n",
    "#BERT's implementation comes with a pretained tokenizer and a defined vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "#tokenizing the text \n",
    "tokenized_texts = list(map(lambda x: ['[CLS]'] + tokenizer.tokenize(x) + ['[SEP]'] , sentences))\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d3d97d-0186-44c7-ba80-79d8a2ef5f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b110e-28a2-44e0-b8fe-fb57026ca72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing the labels\n",
    "#converting tags to indices \n",
    "tags_vals = list(set(label))  \n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd8fd36-bf92-4354-b73d-79f671847f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now need to give BERT input ids,ie, a sequence of integers which uniquely identify each input token to its index number.\n",
    "#cutting and padding the tokens and labels to our desired length\n",
    "\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15707118-b47d-432c-a1fc-2a6e81bb6e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT supports something called attention masks\n",
    "#Tells the model which tokens should be attended to, and which should not.\n",
    "#learn more about this at https://huggingface.co/transformers/glossary.html#attention-mask\n",
    "\n",
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3f6d9-8224-4b87-b156-93bb8c63e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset to use 20% to validate the model.\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
    "                                                            random_state=2020, test_size=0.2)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d3ad40-6a83-4c83-8a03-768eb6f77e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch requires inputs to be in the form of torch tensors\n",
    "#Learn more about torch tensors at https://pytorch.org/docs/stable/tensors.html\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c7f759-452e-439a-a11f-3936dc420dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Data Loaders\n",
    "#Shuffle the data at training time\n",
    "#Pass them sequentially during test time\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "print(\"Train Data Loaders Ready\")\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n",
    "print(\"Test Data Loaders Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d700986-bdc3-4817-aa27-07e0a657b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertForTokenClassification class of pytorch-pretrained-bert package provides  for token-level predictions\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))#loading pre trained bert\n",
    "print(\"BERT model ready to use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92294da5-31b9-42ef-b64d-2c252ad9545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passing model parameters into GPU\n",
    "if torch.cuda.is_available():    \n",
    "    print(\"Passing Model parameters in GPU\")\n",
    "    print(model.cuda()) \n",
    "else: \n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87560089-5c05-4af7-a1bf-3a4ebb802298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before starting fine tuing we need to add the optimizer. Generally Adam is used\n",
    "#weight_decay is added as regularization to the main weight matrices\n",
    "print(\"Fine Tuning BERT\")\n",
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a5dc1b-f189-422f-b8bf-f1b4d0d664b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad171a-b0d0-4cc9-b44e-8f674ac410d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the epoch number. The bert paper recomends 3-4\n",
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "train_loss_set=[]\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None,\n",
    "                     attention_mask=b_input_mask, labels=b_labels)\n",
    "        train_loss_set.append(loss)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    # VALIDATION on validation set\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                                  attention_mask=b_input_mask, labels=b_labels)\n",
    "            logits = model(b_input_ids, token_type_ids=None,\n",
    "                           attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    eval_loss = eval_loss/nb_eval_steps\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4902db-2faa-424f-8fea-c40f0ee7bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.ylim(0,0.25)\n",
    "plt.plot(train_loss_set)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d06bd-9a64-4290-9886-f0f68ad76f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "for batch in valid_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
    "                              attention_mask=b_input_mask, labels=b_labels)\n",
    "        logits = model(b_input_ids, token_type_ids=None,\n",
    "                       attention_mask=b_input_mask)\n",
    "        \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    true_labels.append(label_ids)\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n",
    "valid_tags = [[tags_vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n",
    "print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
    "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ab0fa-ae86-411d-aa14-c86e3f4e291f",
   "metadata": {},
   "source": [
    "# 6.Named Entity Disambiguation and Linking\n",
    "![](images/entity-link.png)\n",
    "<center>Entity linking by IBM</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d18e78-771b-4b40-8cf0-f031ccb0efe8",
   "metadata": {},
   "source": [
    "*Named entity disambiguation (NED)* refers to the NLP task of\n",
    "achieving exactly this: assigning a unique identity to entities mentioned\n",
    "in the text. It’s also the first step in moving toward more sophisticated\n",
    "tasks to address the scenario mentioned above by identifying\n",
    "relationships between entities. NER and NED together are known as\n",
    "named entity linking (NEL). Some other NLP applications that would\n",
    "need NEL include question answering and constructing large\n",
    "knowledge bases of connected events and entities, such as the Google\n",
    "Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7708e6b-a1c7-4f49-963f-783742591330",
   "metadata": {},
   "source": [
    "## NEL Using Azure API\n",
    "The Azure Text Analytics API is one of the popular APIs for NEL.\n",
    "DBpedia Spotlight is a freely available tool to do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417aab0e-8140-495b-866a-83ddeb4d7e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pprint\n",
    "\n",
    "my_api_key = 'xxxx' #replace this with your api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953738d-ba29-4a70-a00c-3e39d4d24c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_entities(text):\n",
    "    url = \"https://westcentralus.api.cognitive.microsoft.com/text/analytics/v2.1/entities\"\n",
    "    documents = {'documents':[{'id':'1', 'language':'en', 'text':text}]}\n",
    "    headers = {'Ocp-Apim-Subscription-Key': my_api_key}\n",
    "    response = requests.post(url, headers=headers, json=documents)\n",
    "    entities = response.json()\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9ff6e-7073-4983-b270-01b10d966f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = open(\"nytarticle.txt\").read() #This file is in the same folder. \n",
    "entities = print_entities(mytext)\n",
    "for document in entities[\"documents\"]:\n",
    "    pprint.pprint(document[\"entities\"])\n",
    "#This above code will print you a whole lot of stuff you may or may not use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f68cb5f-fdd9-4d07-90ef-96c172a15e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us clean up a little bit, and not print the whole lot of messy stuff it gives us?\n",
    "for document in entities['documents']:\n",
    "    print(\"Entities in this document: \")\n",
    "    for entity in document['entities']:\n",
    "        if entity['type'] in [\"Person\", \"Location\", \"Organization\"]:\n",
    "            print(entity['name'], \"\\t\", entity['type'])\n",
    "            if 'wikipediaUrl' in entity.keys():\n",
    "                print(entity['wikipediaUrl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b289622b-660f-4f8e-932b-c96423f3120e",
   "metadata": {},
   "source": [
    "There are a few important things to keep in mind while using\n",
    "NEL in your project:\n",
    "- Existing NEL approaches are not perfect, and they’re unlikely\n",
    "to fare well with new names or domain-specific terms. Since\n",
    "NEL also requires further linguistic processing, including\n",
    "syntactic parsing, its accuracy is also affected by how well\n",
    "the different processing steps are done.\n",
    "- Like with other IE tasks, the first step in any NLP pipeline—\n",
    "text extraction and cleanup—affects what we see as output for\n",
    "NEL as well. When we use third-party services, we have\n",
    "little control over adapting them to our domain, if needed, or\n",
    "understanding their internal workings to modify them to our\n",
    "needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0bfc8e-480e-4de6-b22e-2ad8a9fa9a44",
   "metadata": {},
   "source": [
    "# 7. Relationship Extraction\n",
    "*Relationship extraction (RE)* is the IE task that deals with extracting\n",
    "entities and relationships between them from text documents. It’s an\n",
    "important step in building a knowledge base, and it’s also useful in\n",
    "improving search and developing question-answering systems.\n",
    "\n",
    "## Approaches to RE\n",
    "RE is often treated as a supervised classification problem. The\n",
    "datasets used to train RE systems contain a set of pre-defined\n",
    "relations, similar to classification datasets. This consists of modeling\n",
    "it as a two-step classification problem:\n",
    "1. Whether two entities in a text are related (binary\n",
    "classification).\n",
    "2. If they are related, what is the relation between them\n",
    "(multiclass classification)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae6fd2-7214-4192-ab08-4ba444737ee3",
   "metadata": {},
   "source": [
    "## RE with the Watson API\n",
    "RE is a hard problem, and it would be challenging and time consuming\n",
    "to develop our own relation extraction systems from scratch. A\n",
    "solution commonly used in NLP projects in the industry is to rely on\n",
    "the Natural Language Understanding service provided by IBM Watson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6accf2e5-bdbe-4ac7-a26c-43bb496a312d",
   "metadata": {},
   "source": [
    "# 8. Other Advanced IE Tasks\n",
    "## Temporal Information Extraction\n",
    "![](images/temporal-event.png)\n",
    "<center>Identifying and extracting temporal events from emails</center>\n",
    "\n",
    "## Event Extraction\n",
    "![](images/life-event.png)\n",
    "<center>Examples of extracting life events from Twitter data</center>\n",
    "\n",
    "## Template Filling\n",
    "![](images/template-filling.png)\n",
    "<center>Example of template filling</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a58af-1921-4779-9089-878252cff5e8",
   "metadata": {},
   "source": [
    "# 9. Case Study\n",
    "![](images/meeting-extraction.png)\n",
    "<center>Meeting information extraction from email (representative image)</center>\n",
    "\n",
    "![](images/pipeline-meeting-extraction.png)\n",
    "<center>Pipeline for meeting information extraction system development</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff0e939-89f7-4ea1-8ae2-7e58e9088ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
