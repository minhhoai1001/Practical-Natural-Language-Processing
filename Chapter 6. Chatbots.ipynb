{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c1a99f-a0fe-499b-95c1-280627bff2b4",
   "metadata": {},
   "source": [
    "# 1. Applications\n",
    "- Shopping and e-commerce\n",
    "- News and content discovery\n",
    "- Customer service\n",
    "- Medical\n",
    "- Legal\n",
    "\n",
    "## A Simple FAQ (frequently asked questions) Bot\n",
    "*Amazon ML FAQ to be used for a FAQ bot*\n",
    "\n",
    "| Questions                                                                                                               | Answer                                                                                                                                                                                                                                                      |\n",
    "|-------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| What can I do with Amazon Machine Learning? How can I use Amazon Machine Learning? What can Amazon Machine Learning do? | You can use Amazon Machine Learning to create a wide variety of predictive applications. For example, you can use Amazon Machine Learning to help you build applications that flag suspicious transactions, detect fraudulent orders, forecast demand, etc. |\n",
    "| What algorithm does Amazon Machine Learning use to generate models? How does Amazon Machine Learning build models?      | Amazon Machine Learning currently uses an industry\u0002standard logistic regression algorithm to generate models.                                                                                                                                                |\n",
    "| Are there limits to the size of the dataset I can use for training? What is the maximum size of training dataset?       | Amazon Machine Learning can train models on datasets up to 100 GB in size.                                                                                                                                                                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a965976-62e1-412f-8f7a-836c2ff3b9ed",
   "metadata": {},
   "source": [
    "# 2. A Taxonomy of Chatbots\n",
    "- Exact answer or FAQ bot with limited conversations\n",
    "- Flow-based bot\n",
    "- Open-ended bot\n",
    "\n",
    "![](images/typeofchatbot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7a1d0e-9330-419e-bdb2-fe1e40a8eef1",
   "metadata": {},
   "source": [
    "## Goal-Oriented Dialog:\n",
    "The natural human purpose of having a conversation is to accomplish a goal via relevant information seeking.\n",
    "\n",
    "## Chitchats:\n",
    "Humans also engage in\n",
    "unstructured, open-domain conversations without any specific goals.\n",
    "These human-human conversations involve free-form, opinionated\n",
    "discussions about various topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afae9d41-79ad-480d-89f2-f345395f9b3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. A Pipeline for Building Dialog Systems\n",
    "![](images/pipefordiago.png)\n",
    "\n",
    "<center>Pipeline for a dialog system</center>\n",
    "\n",
    "- Speech recognition algorithms transcribe speech to natural text.\n",
    "- Natural language understanding (NLU): the system tries to analyze and “understand” the\n",
    "transcribed text.\n",
    "- Dialog and task manager:  gathers and systematically decides which\n",
    "pieces of information are important or not\n",
    "- Natural language generation: decides a strategy for responding,\n",
    "the natural language generation module generates a response in a\n",
    "human-readable form according to the strategy devised by the\n",
    "dialog manager."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911afaa8-e4cb-4230-9b59-fb529883c398",
   "metadata": {},
   "source": [
    "# 4. Dialog Systems in Detail\n",
    "- Dialog act or intent: This is the aim of a user command\n",
    "- Slot or entity: holds information\n",
    "regarding specific entities related to the intent.\n",
    "- Dialog state or context: ontological construct that contains both the\n",
    "information about the dialog act as well as state-value pairs.\n",
    "\n",
    "![](images/terminology.png)\n",
    "<center>Example of different terminology used in chatbots</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f91de61-26dc-4d0c-9051-f3edf498edc5",
   "metadata": {},
   "source": [
    "## PizzaStop Chatbot\n",
    "Dialogflow is a conversational agent–building platform by Google. By\n",
    "providing the tools to understand and generate natural language and\n",
    "manage the conversation.\n",
    "\n",
    "### BUILDING OUR DIALOGFLOW AGENT\n",
    "1. First, we need to create an agent.\n",
    "![](images/create-dialogflow.png)\n",
    "<center>Creating an agent using Dialogflow</center>\n",
    "\n",
    "2. You’ll then be redirected to another page with options that\n",
    "allow you to create the bot.\n",
    "\n",
    "![](images/dialogflow-UI.png)\n",
    "<center>Dialogflow UI after creating an agent</center>\n",
    "\n",
    "3. Now, we need to add the intents and entities we care about to\n",
    "our agent.\n",
    "4. Now, we’ll create the first intent: orderPizza. As we create a\n",
    "new intent, we have to provide training examples, called\n",
    "“training phrases,” to enable the bot to detect variations of\n",
    "responses that belong to the intent.\n",
    "5. Since we’ve included intent, we need to add the respective\n",
    "entities to remember important information provided by the\n",
    "user. Create an entity named pizzaSize, enable “fuzzy\n",
    "matching” (which matches entities even if they’re only\n",
    "approximately the same), and provide the necessary values.\n",
    "6. Now, let’s go back to the Intents block to add additional\n",
    "information to the Action and Parameters section.\n",
    "7. We also need to provide sample responses.\n",
    "8. So far, we’ve added a simple intent and entities.\n",
    "9. We can add many more intents and entities to make our agent\n",
    "robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d03e6-90bf-42fe-8607-a26bc3675fe3",
   "metadata": {},
   "source": [
    "### TESTING OUR AGENT\n",
    "![](images/test_pizza-oder.png)\n",
    "<center>Making a simple order using our agent</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82395aa2-c430-44de-9011-20006751271e",
   "metadata": {},
   "source": [
    "# 5. Deep Dive into Components of a Dialog System\n",
    "![](images/restaurent-booking.png)\n",
    "<center>Conversation about restaurant booking</center>\n",
    "\n",
    "## Dialog Act Classification\n",
    "Dialog act classification is a task to identify how the user utterance\n",
    "plays a role in the context of dialog. Identifying intent helps to understand what the user is asking for and to\n",
    "take actions accordingly.\n",
    "\n",
    "## Identifying Slots\n",
    "Once we’ve extracted the intents, we want to move on to extracting\n",
    "entities. Extracting entities is also important for generating correct and\n",
    "appropriate responses to the user’s input. \n",
    "\n",
    "## Response Generation\n",
    "- Fixed responses\n",
    "- Use of templates\n",
    "- Automatic generation\n",
    "\n",
    "## Dialog Examples with Code Walkthrough\n",
    "### DATASETS\n",
    "Table. Goal-oriented datasets from various domains and\n",
    "their usage\n",
    "\n",
    "| Dataset  | Domain           | Usage                                                                                                                                                                                                                               |\n",
    "|----------|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| ATIS     | AirTicketBooking | Benchmark for intent classification and slot filling. This is a single-domain dataset, hence entities and intents are restricted to one domain.                                                                                     |\n",
    "| SNIPS    | Multidomain      | Benchmark for intent classification and slot filling. This is a multidomain dataset, hence the entities belong to multiple domains. Multiple-domain datasets are challenging to model due their variability.                        |\n",
    "| DSTC     | Restaurants      | Benchmark for dialog state tracking or joint determination of intent and slots. This is similarly a single-domain dataset, but the entities are expressed more in terms of annotations and contain more metadata.                   |\n",
    "| MultiWoZ | Multidomain      | Benchmark for dialog state tracking or joint determination of intent and slots that spans over multiple domains. For the similar reason of variability, modeling this dataset is more challenging than modeling single domain ones. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6586f9ed-27a6-49aa-a8a9-872a36352195",
   "metadata": {},
   "source": [
    "### DIALOG ACT PREDICTION\n",
    "In this notebook we demonstrate various CNN and RNN architectures for the task of intent detection on the ATIS dataset. The ATIS dataset is a standard benchmark dataset for the tast of intent detection. ATIS Stands for Airline Travel Information System. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c67921d-278a-4598-b4f0-264ed7f8961c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a6abfb4-03a7-41cf-b406-0a0ba3217334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general imports\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "random.seed(0) #for reproducability of results\n",
    "\n",
    "#basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#NN imports\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras.initializers import Constant\n",
    "\n",
    "#encoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084838d7-bdb6-4a8f-b92f-15d493cb0284",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "We load the data with the help of a few functions from utils.py which is included in this repository's Ch6 folder under folder name \"Data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04a9117a-066e-4a36-b87d-ac4c71d65ad7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data\\\\data2\\x07tis.train.w-intent.iob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Training Data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_data, read_method\n\u001b[1;32m----> 4\u001b[0m sents,labels,intents \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata2\u001b[39;49m\u001b[38;5;130;43;01m\\a\u001b[39;49;00m\u001b[38;5;124;43mtis.train.w-intent.iob\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m train_sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m sents]\n\u001b[0;32m      8\u001b[0m train_texts \u001b[38;5;241m=\u001b[39m train_sentences\n",
      "File \u001b[1;32m~\\Documents\\dev\\NLP\\data\\utils.py:31\u001b[0m, in \u001b[0;36mfetch_data\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_data\u001b[39m(fname):\n\u001b[1;32m---> 31\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[43mread_method\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(fname)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'data\\\\data2\\x07tis.train.w-intent.iob'"
     ]
    }
   ],
   "source": [
    "# Training Data\n",
    "from data.utils import fetch_data, read_method\n",
    "\n",
    "sents,labels,intents = fetch_data('data\\data2\\atis.train.w-intent.iob')\n",
    "\n",
    "train_sentences = [\" \".join(i) for i in sents]\n",
    "\n",
    "train_texts = train_sentences\n",
    "train_labels= intents.tolist()\n",
    "\n",
    "vals = []\n",
    "\n",
    "for i in range(len(train_labels)):\n",
    "    if \"#\" in train_labels[i]:\n",
    "        vals.append(i)\n",
    "        \n",
    "for i in vals[::-1]:\n",
    "    train_labels.pop(i)\n",
    "    train_texts.pop(i)\n",
    "\n",
    "print (\"Number of training sentences :\",len(train_texts))\n",
    "print (\"Number of unique intents :\",len(set(train_labels)))\n",
    "\n",
    "for i in zip(train_texts[:5], train_labels[:5]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12d4f5-0f1f-414f-8760-69c6fb2e84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Data\n",
    "from data.utils import fetch_data, read_method\n",
    "\n",
    "sents,labels,intents = fetch_data('data/data2/atis.test.w-intent.iob')\n",
    "\n",
    "test_sentences = [\" \".join(i) for i in sents]\n",
    "\n",
    "test_texts = test_sentences\n",
    "test_labels = intents.tolist()\n",
    "\n",
    "new_labels = set(test_labels) - set(train_labels)\n",
    "\n",
    "vals = []\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    if \"#\" in test_labels[i]:\n",
    "        vals.append(i)\n",
    "    elif test_labels[i] in new_labels:\n",
    "        print(test_labels[i])\n",
    "        vals.append(i)\n",
    "        \n",
    "for i in vals[::-1]:\n",
    "    test_labels.pop(i)\n",
    "    test_texts.pop(i)\n",
    "\n",
    "print (\"Number of testing sentences :\",len(test_texts))\n",
    "print (\"Number of unique intents :\",len(set(test_labels)))\n",
    "\n",
    "for i in zip(test_texts[:5], test_labels[:5]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49cec3a-90f5-4c8a-b880-610ef6cbe325",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c57468-7082-48be-b238-96e275922a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 300\n",
    "MAX_NUM_WORDS = 20000 \n",
    "EMBEDDING_DIM = 100 \n",
    "VALIDATION_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ea516-6ff9-42bf-8916-b743b0b7870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts) #Converting text to a vector of word indexes\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefc44a9-b25e-48b6-93ce-34d0cd96a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "test_labels = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11733a9-7de3-4b91-99f4-d82c26c106f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\n",
    " #initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\n",
    "trainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "trainvalid_labels = to_categorical(train_labels)\n",
    "\n",
    "test_labels = to_categorical(np.asarray(test_labels), num_classes= trainvalid_labels.shape[1])\n",
    "\n",
    "# split the training data into a training set and a validation set\n",
    "indices = np.arange(trainvalid_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "trainvalid_data = trainvalid_data[indices]\n",
    "trainvalid_labels = trainvalid_labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\n",
    "x_train = trainvalid_data[:-num_validation_samples]\n",
    "y_train = trainvalid_labels[:-num_validation_samples]\n",
    "x_val = trainvalid_data[-num_validation_samples:]\n",
    "y_val = trainvalid_labels[-num_validation_samples:]\n",
    "#This is the data we will use for CNN and RNN training\n",
    "print('Splitting the train data into train and valid is done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27738431-62f4-4f85-be2e-ba97ca28270d",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "Embedding Matrix\n",
    "\n",
    "We need to prepare our embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efe9d17-b3b1-4020-bd40-e0ed59f407d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "# Download GloVe 6B from here: https://nlp.stanford.edu/projects/glove/\n",
    "BASE_DIR = 'Data'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n",
    "#print(embeddings_index[\"google\"])\n",
    "\n",
    "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load these pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "print(\"Preparing of embedding matrix is done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2434f-2485-4db4-bf76-68cf733b9de4",
   "metadata": {},
   "source": [
    "### CNN with Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a955572-1f62-402e-b17f-c7aaff8e1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Define a 1D CNN model.')\n",
    "\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(embedding_layer)\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(GlobalMaxPooling1D())\n",
    "cnnmodel.add(Dense(128, activation='relu'))\n",
    "cnnmodel.add(Dense(len(trainvalid_labels[0]), activation='softmax'))\n",
    "\n",
    "cnnmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "cnnmodel.summary()\n",
    "\n",
    "#Train the model. Tune to validation set. \n",
    "cnnmodel.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1, validation_data=(x_val, y_val))\n",
    "#Evaluate on test set:\n",
    "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
    "print('Test accuracy with CNN:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5525cc9-646e-4554-b377-82beef4c76cb",
   "metadata": {},
   "source": [
    "### CNN-Embedding Layer\n",
    "Here, we train a CNN model with an embedding layer which is being trained on the fly instead of using the pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8c9d1a-8a9b-4fda-af6e-17a3357166c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\")\n",
    "cnnmodel = Sequential()\n",
    "cnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(MaxPooling1D(5))\n",
    "cnnmodel.add(Conv1D(128, 5, activation='relu'))\n",
    "cnnmodel.add(GlobalMaxPooling1D())\n",
    "cnnmodel.add(Dense(128, activation='relu'))\n",
    "cnnmodel.add(Dense(len(trainvalid_labels[0]), activation='softmax'))\n",
    "\n",
    "cnnmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "cnnmodel.summary()\n",
    "\n",
    "#Train the model. Tune to validation set. \n",
    "cnnmodel.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1, validation_data=(x_val, y_val))\n",
    "#Evaluate on test set:\n",
    "score, acc = cnnmodel.evaluate(test_data, test_labels)\n",
    "print('Test accuracy with CNN:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3deae6-f8cd-48b1-9d72-dbbb77abdfae",
   "metadata": {},
   "source": [
    "### RNN-Embedding Layer\n",
    "Here, we train a RNN model with an embedding layer which is being trained on the fly instead of using the pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33360ee2-d2fc-4d79-bec6-d028bcb235e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Defining and training an LSTM model, training embedding layer on the fly\")\n",
    "\n",
    "#modified from: \n",
    "\n",
    "rnnmodel = Sequential()\n",
    "rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
    "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "rnnmodel.add(Dense(len(trainvalid_labels[0]), activation='sigmoid'))\n",
    "rnnmodel.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "rnnmodel.summary()\n",
    "\n",
    "print('Training the RNN')\n",
    "rnnmodel.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=1,\n",
    "          validation_data=(x_val, y_val))\n",
    "score, acc = rnnmodel.evaluate(test_data, test_labels,\n",
    "                            batch_size=32)\n",
    "print('Test accuracy with RNN:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2260c1-47d1-4965-a2a3-0d7af4d30895",
   "metadata": {},
   "source": [
    "### LSTM with Pre-Trained Embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54eab86-6fc1-4680-bbb0-85a05b473ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Defining and training an LSTM model, using pre-trained embedding layer\")\n",
    "\n",
    "#modified from: \n",
    "\n",
    "rnnmodel2 = Sequential()\n",
    "rnnmodel2.add(embedding_layer)\n",
    "rnnmodel2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "rnnmodel2.add(Dense(len(trainvalid_labels[0]), activation='sigmoid'))\n",
    "rnnmodel2.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "rnnmodel2.summary()\n",
    "\n",
    "print('Training the RNN')\n",
    "rnnmodel2.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=1,\n",
    "          validation_data=(x_val, y_val))\n",
    "score, acc = rnnmodel2.evaluate(test_data, test_labels,\n",
    "                            batch_size=32)\n",
    "print('Test accuracy with RNN:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a01007-5408-47fa-9aee-41f48c5c155a",
   "metadata": {},
   "source": [
    "# 6. Other Dialog Pipelines\n",
    "## End-to-End Approach\n",
    "we can build a chatbot using seq2seq models.\n",
    "Imagine that the input of the model is the user utterance: a sequence of\n",
    "words. As the output, it generates another sequence of words, which is\n",
    "the response from the bot. Seq2seq models are end-to-end trainable, so\n",
    "we don’t have to maintain multiple modules, and they are generally\n",
    "LSTM based.\n",
    "\n",
    "## Deep Reinforcement Learning for Dialogue Generation\n",
    "You can see that the reinforcement learning–based model generated a\n",
    "more diverse response instead of collapsing into a generic default\n",
    "response.\n",
    "\n",
    "![](images/reinforcement.png)\n",
    "<center>Comparison of deep reinforcement learning and a seq2seq model</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27125740-8888-4b60-8738-5fedd171d800",
   "metadata": {},
   "source": [
    "## Human-in-the-Loop\n",
    "The machine may\n",
    "improve its performance if humans intervene in its learning process\n",
    "and reward or penalize based on the correct or incorrect response.\n",
    "These rewards or penalties act as feedback for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2362cae-1f5f-4312-bf1e-4a1b0499e5bb",
   "metadata": {},
   "source": [
    "# 7. Rasa NLU\n",
    "![](images/rasa.png)\n",
    "<center>Rasa chatbot interface and interactive learning framework</center>\n",
    "\n",
    "- Context-based conversations\n",
    "- Interactive learning\n",
    "- Data annotation\n",
    "- API integration\n",
    "- Customize your models in Rasa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c30872-88fd-4da6-a362-d0e931dcd55d",
   "metadata": {},
   "source": [
    "# 8. A Case Study: Recipe Recommendations\n",
    "## Utilizing Existing Frameworks\n",
    "We’ll start with Dialogflow, the cloud API. Before we start, we need to define\n",
    "entities like we did before, such as ingredients, cuisine, calorie level,\n",
    "cooking time. We can build an ontology for the cooking domain and\n",
    "identify the number of slots we’d like our chatbot to support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6c240-15c5-46bf-9721-35c2d7d73305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
