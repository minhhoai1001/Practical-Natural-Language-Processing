{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7855a2c",
   "metadata": {},
   "source": [
    "# 1.Data acquisition\n",
    "- Use a public dataset\n",
    "- Scrape data\n",
    "- Product intervention\n",
    "- Data augmentation\n",
    "## Data augmentation\n",
    "- Synonym replacement\n",
    "- Back translation\n",
    "- TF-IDF–based word replacement\n",
    "- Bigram flipping\n",
    "- Replacing entities\n",
    "- Adding noise to data\n",
    "- Advanced techniques\n",
    " * Snorkel\n",
    " * Easy Data Augmentation (EDA) and NLPAug\n",
    " * Active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086e0b7a",
   "metadata": {},
   "source": [
    "# 2.Text Extraction and Cleanup\n",
    "## HTML Parsing and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2326e13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the module/method used to get the current time?\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "myurl = \"https://stackoverflow.com/questions/415511/how-to-get-the-current-time-in-python\"\n",
    "html = urlopen(myurl).read()\n",
    "\n",
    "soupified = BeautifulSoup(html, \"html.parser\")\n",
    "question = soupified.find(\"div\", {\"class\": \"question\"})\n",
    "questiontext = question.find(\"div\", {\"class\": \"s-prose js-post-body\"})\n",
    "\n",
    "print(\"Question: \\n\", questiontext.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "128af7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best answer: \n",
      " Use:\n",
      ">>> import datetime\n",
      ">>> datetime.datetime.now()\n",
      "datetime.datetime(2009, 1, 6, 15, 8, 24, 78915)\n",
      "\n",
      ">>> print(datetime.datetime.now())\n",
      "2009-01-06 15:08:24.789150\n",
      "\n",
      "And just the time:\n",
      ">>> datetime.datetime.now().time()\n",
      "datetime.time(15, 8, 24, 78915)\n",
      "\n",
      ">>> print(datetime.datetime.now().time())\n",
      "15:08:24.789150\n",
      "\n",
      "See the documentation for more information.\n",
      "To save typing, you can import the datetime object from the datetime module:\n",
      ">>> from datetime import datetime\n",
      "\n",
      "Then remove the leading datetime. from all of the above.\n"
     ]
    }
   ],
   "source": [
    "answer = soupified.find(\"div\", {\"class\": \"answer\"})\n",
    "answertext = answer.find(\"div\", {\"class\": \"s-prose js-post-body\"})\n",
    "\n",
    "print(\"Best answer: \\n\", answertext.get_text().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d521446",
   "metadata": {},
   "source": [
    "## Unicode Normalization\n",
    "b'I love Pizza \\xf0\\x9f\\x8d\\x95! Shall we book a cab \\xf0\\x9f\\x9a\\x95\n",
    "to get pizza?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c191359",
   "metadata": {},
   "source": [
    "## Spelling Correction\n",
    "Shorthand typing: *Hllo world! I am back!* \\\n",
    "Fat finger problem [20]: *I pronise that I will not bresk the silence\n",
    "again!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2737994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"error\": {\n",
      "        \"code\": \"401\",\n",
      "        \"message\": \"Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "api_key = \"<ENTER-KEY-HERE>\"\n",
    "example_text = \"Hollo, wrld\" # the text to be spell-checked\n",
    "endpoint = \"https://api.cognitive.microsoft.com/bing/v7.0/SpellCheck\"\n",
    "\n",
    "data = {'text': example_text}\n",
    "params = {\n",
    "    'mkt':'en-us',\n",
    "    'mode':'proof'\n",
    "    }\n",
    "headers = {\n",
    "    'Content-Type': 'application/x-www-form-urlencoded',\n",
    "    'Ocp-Apim-Subscription-Key': api_key,\n",
    "    }\n",
    "\n",
    "response = requests.post(endpoint, headers=headers, params=params, data=data)\n",
    "\n",
    "json_response = response.json()\n",
    "print(json.dumps(json_response, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb5a84d",
   "metadata": {},
   "source": [
    "```\n",
    "Output (partially shown here):\n",
    "\"suggestions\": [\n",
    "    {\n",
    "    \"suggestion\": \"Hello\",\n",
    "    \"score\": 0.9115257530801\n",
    "    },\n",
    "    {\n",
    "    \"suggestion\": \"Hollow\",\n",
    "    \"score\": 0.858039839213461\n",
    "    },\n",
    "    {\n",
    "    \"suggestion\": \"Hallo\",\n",
    "    \"score\": 0.597385084464481\n",
    "    }\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce6859",
   "metadata": {},
   "source": [
    "## System-Specific Error Correction\n",
    "Consider another scenario where our dataset is in the\n",
    "form of a collection of PDF documents. While there are several libraries, such as **PyPDF**,\n",
    "**PDFMiner**, etc., to extract text from PDF documents. \\\n",
    "Another common source of textual data is scanned documents. Text\n",
    "extraction from scanned documents is typically done through **optical\n",
    "character recognition (OCR)**, using libraries such as **Tesseract**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1579077",
   "metadata": {},
   "source": [
    "# 3. Pre-Processing\n",
    "- **Preliminaries**: Sentence segmentation and word tokenization.\n",
    "- **Frequent steps**: Stop word removal, stemming and lemmatization, removing digits/punctuation, lowercasing, etc.\n",
    "- **Other steps**: Normalization, language detection, code mixing, transliteration, etc.\n",
    "- **Advanced processing**: POS tagging, parsing, coreference resolution, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b532f50",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "### SENTENCE SEGMENTATION\n",
    "As a simple rule, we can do sentence segmentation by breaking up text\n",
    "into sentences at the appearance of full stops and question marks.\n",
    "A commonly used library is Natural Language\n",
    "Tool Kit (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0227eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\minhh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "mytext = \"In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life. If we were asked to build such an application, think about how we would approach doing so at our organization. We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them. Since language processing is involved, we would also list all the forms of text processing needed at each step. This step-by-step processing of text is known as pipeline. It is the series of steps involved in building any NLP model. These steps are common in every NLP project, so it makes sense to study them in this chapter. Understanding some common procedures in any NLP pipeline will enable us to get started on any NLP problem encountered in the workplace. Laying out and developing a text-processing pipeline is seen as a starting point for any NLP application development process. In this hapter, we will learn about the various steps involved and how they play important roles in solving the NLP problem and we’ll see a few guidelines about when and how to use which step. In later chapters, we’ll discuss specific pipelines for various NLP tasks (e.g., Chapters 4–7).\"\n",
    "\n",
    "my_sentences = sent_tokenize(mytext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cffbd9-8ca1-4256-9a3d-6387c941963a",
   "metadata": {},
   "source": [
    "### WORD TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03d9c764-7293-442b-afb9-67de6a2177dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the previous chapter, we saw examples of some common NLP applications that we might encounter in everyday life.\n",
      "['In', 'the', 'previous', 'chapter', ',', 'we', 'saw', 'examples', 'of', 'some', 'common', 'NLP', 'applications', 'that', 'we', 'might', 'encounter', 'in', 'everyday', 'life', '.']\n",
      "If we were asked to build such an application, think about how we would approach doing so at our organization.\n",
      "['If', 'we', 'were', 'asked', 'to', 'build', 'such', 'an', 'application', ',', 'think', 'about', 'how', 'we', 'would', 'approach', 'doing', 'so', 'at', 'our', 'organization', '.']\n",
      "We would normally walk through the requirements and break the problem down into several sub-problems, then try to develop a step-by-step procedure to solve them.\n",
      "['We', 'would', 'normally', 'walk', 'through', 'the', 'requirements', 'and', 'break', 'the', 'problem', 'down', 'into', 'several', 'sub-problems', ',', 'then', 'try', 'to', 'develop', 'a', 'step-by-step', 'procedure', 'to', 'solve', 'them', '.']\n",
      "Since language processing is involved, we would also list all the forms of text processing needed at each step.\n",
      "['Since', 'language', 'processing', 'is', 'involved', ',', 'we', 'would', 'also', 'list', 'all', 'the', 'forms', 'of', 'text', 'processing', 'needed', 'at', 'each', 'step', '.']\n",
      "This step-by-step processing of text is known as pipeline.\n",
      "['This', 'step-by-step', 'processing', 'of', 'text', 'is', 'known', 'as', 'pipeline', '.']\n",
      "It is the series of steps involved in building any NLP model.\n",
      "['It', 'is', 'the', 'series', 'of', 'steps', 'involved', 'in', 'building', 'any', 'NLP', 'model', '.']\n",
      "These steps are common in every NLP project, so it makes sense to study them in this chapter.\n",
      "['These', 'steps', 'are', 'common', 'in', 'every', 'NLP', 'project', ',', 'so', 'it', 'makes', 'sense', 'to', 'study', 'them', 'in', 'this', 'chapter', '.']\n",
      "Understanding some common procedures in any NLP pipeline will enable us to get started on any NLP problem encountered in the workplace.\n",
      "['Understanding', 'some', 'common', 'procedures', 'in', 'any', 'NLP', 'pipeline', 'will', 'enable', 'us', 'to', 'get', 'started', 'on', 'any', 'NLP', 'problem', 'encountered', 'in', 'the', 'workplace', '.']\n",
      "Laying out and developing a text-processing pipeline is seen as a starting point for any NLP application development process.\n",
      "['Laying', 'out', 'and', 'developing', 'a', 'text-processing', 'pipeline', 'is', 'seen', 'as', 'a', 'starting', 'point', 'for', 'any', 'NLP', 'application', 'development', 'process', '.']\n",
      "In this hapter, we will learn about the various steps involved and how they play important roles in solving the NLP problem and we’ll see a few guidelines about when and how to use which step.\n",
      "['In', 'this', 'hapter', ',', 'we', 'will', 'learn', 'about', 'the', 'various', 'steps', 'involved', 'and', 'how', 'they', 'play', 'important', 'roles', 'in', 'solving', 'the', 'NLP', 'problem', 'and', 'we', '’', 'll', 'see', 'a', 'few', 'guidelines', 'about', 'when', 'and', 'how', 'to', 'use', 'which', 'step', '.']\n",
      "In later chapters, we’ll discuss specific pipelines for various NLP tasks (e.g., Chapters 4–7).\n",
      "['In', 'later', 'chapters', ',', 'we', '’', 'll', 'discuss', 'specific', 'pipelines', 'for', 'various', 'NLP', 'tasks', '(', 'e.g.', ',', 'Chapters', '4–7', ')', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in my_sentences:\n",
    "    print(sentence)\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d81d5fb-61d7-488a-aef1-da5027206a79",
   "metadata": {},
   "source": [
    "![img](images/tokenization.svg)\n",
    "<center>Language-specific (English here) exceptions during tokenization</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b04d1-f332-4c61-8bef-aae053152b28",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Frequent Steps\n",
    "The code example below shows how to remove stop words, digits,\n",
    "and punctuation and lowercase a given collection of texts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c5f9ff-31f9-437d-a264-3dfb923f070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess_corpus(texts):\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    def remove_stops_digits(tokens):\n",
    "        return [token.lower() for token in tokens if token not in mystopwords\n",
    "            and not token.isdigit() and token not in punctuation]\n",
    "    \n",
    "    return [remove_stops_digits(word_tokenize(text)) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ada768-fb60-49f3-820f-08e7491ae2e7",
   "metadata": {},
   "source": [
    "### STEMMING AND LEMMATIZATION\n",
    "Stemming refers to the process of removing suffixes and reducing a\n",
    "word to some base form such that all different variants of that word\n",
    "can be represented by the same form (e.g., “car” and “cars” are both\n",
    "reduced to “car”). \\\n",
    "The following code snippet shows how to use a popular stemming\n",
    "algorithm called **Porter Stemmer** using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a9918cd-0481-408e-a07c-137af652ee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car revolut\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "word1, word2 = \"cars\", \"revolution\"\n",
    "print(stemmer.stem(word1), stemmer.stem(word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b5631e-cfaa-4d62-9814-af5892b63322",
   "metadata": {},
   "source": [
    "Lemmatization is the process of mapping all the different forms of a\n",
    "word to its base word, or lemma. While this seems close to the\n",
    "definition of stemming, they are, in fact, different. For example, the\n",
    "adjective “better,” when stemmed, remains the same. However, upon\n",
    "lemmatization, this should become “good”.\n",
    "\n",
    "![](images/stemming-lemmatization.png)\n",
    "<center>Difference between stemming and lemmatization</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c471883-2b99-4d19-b8ad-8d9d966d75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) #a is for adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc0c3f5-e108-4a32-8212-aad74618786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "token = sp(u'better')\n",
    "for word in token:\n",
    "print(word.text, word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7dc6e-30d9-4c84-b52c-e7756f998f61",
   "metadata": {},
   "source": [
    "NLTK prints the output as “good,” whereas spaCy prints “well” both\n",
    "are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b9ad15-01f1-4b75-9292-f28b491ea6dd",
   "metadata": {},
   "source": [
    "### Other Pre-Processing Steps\n",
    "### TEXT NORMALIZATION\n",
    "Consider a scenario where we’re working with a collection of social\n",
    "media posts to detect news events. Social media text is very different\n",
    "from the language we’d see in, say, newspapers. A word can be\n",
    "spelled in different ways, including in shortened forms, a phone\n",
    "number can be written in different formats (e.g., with and without\n",
    "hyphens), names are sometimes in lowercase, and so on. When we’re\n",
    "working on developing NLP tools to work with such data, it’s useful to\n",
    "reach a canonical representation of text that captures all these\n",
    "variations into one representation. This is known as text\n",
    "normalization.\n",
    "\n",
    "### LANGUAGE DETECTION\n",
    "We can use libraries like **Polyglot** for\n",
    "language detection.\n",
    "\n",
    "### CODE MIXING AND TRANSLITERATION\n",
    "There’s another scenario where a\n",
    "single piece of content is in more than one language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc722730-aa34-4edc-b91a-8523b2eb741f",
   "metadata": {},
   "source": [
    "### Advanced Processing\n",
    "Identifying names requires us to be\n",
    "able to do POS tagging, as identifying proper nouns can be useful in\n",
    "identifying person and organization names. \\\n",
    "Pre-trained and readily usable POS\n",
    "taggers are implemented in NLP libraries such as NLTK, spaCy,\n",
    "and Parsey McParseface Tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8562fac6-67e4-4264-8738-b23a884255b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charles Charles PROPN Xxxxx True False\n",
      "Spencer Spencer PROPN Xxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "was be AUX xxx True True\n",
      "born bear VERB xxxx True False\n",
      "on on ADP xx True True\n",
      "16 16 NUM dd False False\n",
      "April April PROPN Xxxxx True False\n",
      "1889 1889 NUM dddd False False\n",
      "toHannah toHannah PROPN xxXxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "( ( PUNCT ( False False\n",
      "born bear VERB xxxx True False\n",
      "Hannah Hannah PROPN Xxxxx True False\n",
      "Harriet Harriet PROPN Xxxxx True False\n",
      "Pedlingham Pedlingham PROPN Xxxxx True False\n",
      "Hill Hill PROPN Xxxx True False\n",
      ") ) PUNCT ) False False\n",
      "and and CCONJ xxx True True\n",
      "Charles Charles PROPN Xxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "Sr Sr PROPN Xx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Charles Spencer Chaplin was born on 16 April 1889 toHannah Chaplin (born Hannah Harriet Pedlingham Hill) and Charles Chaplin Sr')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_,\n",
    "    token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc18bd10-0939-461f-ab99-5143d9125414",
   "metadata": {},
   "source": [
    "![](images/output.png)\n",
    "<center>Output from different stages of NLP pipeline processing</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9332a6c-0fc7-4da4-948c-c8d20e953930",
   "metadata": {},
   "source": [
    "![](images/An-example-of-preprocessing-steps-of-text.pbm)\n",
    "<center>Advanced pre-processing steps on a blob of text</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a84d45f-4f15-4d3c-a3fd-e55ef034ef88",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering\n",
    "The\n",
    "goal of feature engineering is to capture the characteristics of the text\n",
    "into a numeric vector that can be understood by the ML algorithms.\n",
    "\n",
    "![](images/classical-NLP.png)\n",
    "![](images/deep-learning-NLP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf04308-0b8c-40fa-8af3-5fc5b29e90cb",
   "metadata": {},
   "source": [
    "## Classical NLP/ML Pipeline\n",
    "Feature\n",
    "engineering steps convert the raw data into a format that can be\n",
    "consumed by a machine. These transformation functions are usually\n",
    "handcrafted in the classical ML pipeline, aligning to the task at hand.\n",
    "\n",
    "## DL Pipeline\n",
    "Handcrafted feature engineering becomes a bottleneck for both model\n",
    "performance and the model development cycle. A noisy or unrelated\n",
    "feature can potentially harm the model’s performance by adding more\n",
    "randomness to the data. \\\n",
    "In the DL pipeline, the raw data (after pre\u0002processing) is directly fed to a model. The model is capable of\n",
    "“learning” features from the data. Hence, these features are more in\n",
    "line with the task at hand, so they generally give improved\n",
    "performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ead5e-0703-473a-9f70-a9983a6a0b20",
   "metadata": {},
   "source": [
    "# 5. Modeling\n",
    "## Start with Simple Heuristics\n",
    "At the very start of building a model, ML may not play a major role by\n",
    "itself. Part of that could be due to a lack of data, but human-built\n",
    "heuristics can also provide a great start in some ways. Heuristics may\n",
    "already be part of your system, either implicitly or explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23342597-a2f2-4346-a280-e8e265802839",
   "metadata": {},
   "source": [
    "## Building Your Model\n",
    "- Create a feature from the heuristic for your ML model\n",
    "- Pre-process your input to the ML model\n",
    "\n",
    "Additionally, we have NLP service providers, such as Google Cloud\n",
    "Natural Language, Amazon Comprehend, Microsoft Azure\n",
    "Cognitive Services, and IBM Watson Natural Language\n",
    "Understanding, which provide off-the-shelf APIs to solve various\n",
    "NLP tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9683b76b-db79-4386-8177-bd134b974a36",
   "metadata": {},
   "source": [
    "## Building THE Model\n",
    "- Ensemble and stacking\n",
    "- Better feature engineering\n",
    "- Transfer learning\n",
    "- Reapplying heuristics\n",
    "\n",
    "| Data attribute | Decision path | Examples |\n",
    "|---|---|---|\n",
    "| Large data volume | Can use techniques that require more data, like DL. Can use a richer set of features as well. If the data is sufficiently large but unlabeled, we can also apply unsupervised techniques. | If we have a lot of reviews and metadata associated with them, we can build a sentiment-analysis tool from scratch. |\n",
    "| Small data volume | Need to start with rule-based or traditional ML solutions that are less data hungry. Can also adapt cloud APIs and generate more data with weak supervision. We can also use transfer learning if there’s a similar task that has large data. | This often happens at the start of a completely new project. |\n",
    "| Data quality is poor and the data is heterogeneous in nature | More data cleaning and preprocessing might be required. | This entails issues like code mixing (different languages being mixed in the same sentence), unconventional language, transliteration, or noise (like social media text). |\n",
    "| Data quality is good | Can directly apply off-the-shelf algorithms or cloud APIs more easily. | Legal text or newspapers. |\n",
    "| Data consists of full-length documents | Choose the right strategy for breaking the document into lower levels, like paragraphs, sentences, or phrases, depending on the problem. | Document classification, review analysis, etc. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad68b5f-6be6-45ce-9bc7-2042d3025052",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 6.Evaluation\n",
    "Evaluations are of two types: intrinsic and extrinsic. Intrinsic\n",
    "focuses on intermediary objectives, while extrinsic focuses on\n",
    "evaluating performance on the final objective.\n",
    "## Intrinsic Evaluation\n",
    "| Metric | Description  | Applications |\n",
    "|---|---|---|\n",
    "| Accuracy | Used when the output variable is categorical or discrete. It denotes the fraction of times the model makes correct predictions as compared to the total predictions it makes. | Mainly used in classification tasks, such as sentiment classification (multiclass), natural language inference (binary), paraphrase detection (binary), etc. |\n",
    "| Precision | Shows how precise or exact the model’s predictions are, i.e., given all the positive (the class we care about) cases, how many can the model classify correctly? | Used in various classification tasks, especially in cases where mistakes in a positive class are more costly than mistakes in a negative class, e.g., disease predictions in healthcare. |\n",
    "| Recall | Recall is complementary to precision. It captures how well the model can recall positive class, i.e., given all the positive predictions it makes, how many of them are indeed positive? | Used in classification tasks, especially where retrieving positive results is more important, e.g., e-commerce search and other information retrieval tasks. |\n",
    "| F1 score | Combines precision and recall to give a single metric, which also captures the trade-off between precision and recall, i.e., completeness and exactness. F1 is defined as (2 × Precision × Recall) / (Precision + Recall). | Used simultaneously with accuracy in most of the classification tasks. It is also used in sequence-labeling tasks, such as entity extraction, retrieval-based questions answering, etc. |\n",
    "| AUC | Captures the count of positive predictions that are correct versus the count of positive predictions that are incorrect as we vary the threshold for prediction. | Used to measure the quality of a model independent of the prediction threshold. It is used to find the optimal prediction threshold for a classification task. |\n",
    "| MRR (mean reciprocal rank) | Used to evaluate the responses retrieved given their probability of correctness. It is the mean of the reciprocal of the ranks of the retrieved results. | Used heavily in all information-retrieval tasks, including article search, e-commerce search, etc. |\n",
    "| MAP (mean average precision) | Used in ranked retrieval results, like MRR. It calculates the mean precision across each retrieved result. | Used in information-retrieval tasks. |\n",
    "| RMSE (root mean squared error) | Captures a model’s performance in a real-value prediction task. Calculates the square root of the mean of the squared errors for each data point. | Used in conjunction with MAPE in the case of regression problems, from temperature prediction to stock market price prediction. |\n",
    "| MAPE (mean absolute percentage error) | Used when the output variable is a continuous variable. It is the average of absolute percentage error for each data point. | Used to test the performance of a regression model. It is often used in conjunction with RMSE. |\n",
    "| BLEU (bilingual evaluation understudy) | Captures the amount of n-gram overlap between the output sentence and the reference ground truth sentence. It has many variants. | Mainly used in machine translation tasks. Recently adapted to other text generation tasks, such as paraphrase generation and text summarization. |\n",
    "| METEOR | A precision-based metric to measure the quality of text generated. It fixes some of the drawbacks of BLEU, such as exact word matching while calculating precision. METEOR allows synonyms and stemmed words to be matched with the reference word. | Mainly used in machine translation. |\n",
    "| ROUGE | Another metric to compare quality of generated text with respect to a reference text. As opposed to BLEU, it measures recall. | Since it measures recall, it’s mainly used for summarization tasks where it’s important to evaluate how many words a model can recall. |\n",
    "| Perplexity | A probabilistic measure that captures how confused an NLP model is. It’s derived from the cross-entropy in a next word prediction task. | Used to evaluate language models. It can also be used in language-generation tasks, such as dialog generation. |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f4aa80-6788-4838-a1ef-33404c1c6cba",
   "metadata": {},
   "source": [
    "## Extrinsic Evaluation\n",
    "Extrinsic evaluation focuses on evaluating the\n",
    "model performance on the final objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f67aa66-5cad-41ed-9f7e-8b3d713b1d42",
   "metadata": {},
   "source": [
    "# 7.Post-Modeling Phases\n",
    "we move on to the post-modeling phase: deploying, monitoring, and updating the model. We’ll\n",
    "cover these briefly in this section.\n",
    "- Deployment\n",
    "- Monitoring\n",
    "- Model Updating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e14d0-2300-43c2-accc-9006551c40cf",
   "metadata": {},
   "source": [
    "# 8.Working with Other Languages\n",
    "The\n",
    "pipeline for some languages may be very similar to English, whereas\n",
    "some languages and scenarios may require us to rethink how we\n",
    "approach the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c99fe54-3023-4c3a-9268-652cfca238f6",
   "metadata": {},
   "source": [
    "# 9. Case Study\n",
    "Let’s see a case study, using Uber’s tool to improve customer care:\n",
    "Customer Obsession Ticketing Assistant (COTA).\n",
    "![](images/Uber-pipeline.png)\n",
    "<center>NLP pipeline for ranking tickets in a ticketing system by Uber</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37285ad-c0ca-4390-a434-0e80d693b9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
